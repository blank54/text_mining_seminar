{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the modules.\n",
    "\n",
    "```\n",
    "pip install config==0.4.2 gensim==3.8.1 gpustat==0.6.0 GPUtil==1.4.0 h5py==2.10.0 JPype1==0.7.1 Keras==2.2.4 konlpy==0.5.2 nltk==3.4.5 numpy==1.18.1 pandas==1.0.1 scikit-learn==0.22.1 scipy==1.4.1 silence-tensorflow==1.1.1 soynlp==0.0.493 tensorflow==1.14.0 tensorflow-gpu==1.14.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the descriptions and install _keras-contrib_.\n",
    "\n",
    "```\n",
    "git clone https://www.github.com/keras-team/keras-contrib.git \n",
    "cd keras-contrib \n",
    "python setup.py install\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore WARNING messages via following modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/blank54/anaconda3/envs/connlp-lstm/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/data/blank54/anaconda3/envs/connlp-lstm/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/data/blank54/anaconda3/envs/connlp-lstm/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/data/blank54/anaconda3/envs/connlp-lstm/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/data/blank54/anaconda3/envs/connlp-lstm/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/data/blank54/anaconda3/envs/connlp-lstm/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/data/blank54/anaconda3/envs/connlp-lstm/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/data/blank54/anaconda3/envs/connlp-lstm/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/data/blank54/anaconda3/envs/connlp-lstm/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/data/blank54/anaconda3/envs/connlp-lstm/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/data/blank54/anaconda3/envs/connlp-lstm/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/data/blank54/anaconda3/envs/connlp-lstm/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from silence_tensorflow import silence_tensorflow\n",
    "silence_tensorflow()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from connlp.analysis import NER_Labels\n",
    "\n",
    "label_dict = {'NON': 0,     #None\n",
    "              'PER': 1,     #PERSON\n",
    "              'FOD': 2,}    #FOOD\n",
    "\n",
    "ner_labels = NER_Labels(label_dict=label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from connlp.preprocess import EnglishTokenizer\n",
    "from connlp.analysis import NER_LabeledSentence, NER_Corpus\n",
    "tokenizer = EnglishTokenizer()\n",
    "\n",
    "data_sents = {'sent1': 'Sam likes pizza',\n",
    "              'sent2': 'Erik eats pizza',\n",
    "              'sent3': 'Erik and Sam are drinking soda',\n",
    "              'sent4': 'Flora cooks chicken',\n",
    "              'sent5': 'Sam ordered a chicken',\n",
    "              'sent6': 'Flora likes chicken sandwitch',\n",
    "              'sent7': 'Erik likes to drink soda'}\n",
    "data_labels = {'sent1': [1, 0, 2],\n",
    "               'sent2': [1, 0, 2],\n",
    "               'sent3': [1, 0, 1, 0, 0, 2],\n",
    "               'sent4': [1, 0, 2],\n",
    "               'sent5': [1, 0, 0, 2],\n",
    "               'sent6': [1, 0, 2, 2],\n",
    "               'sent7': [1, 0, 0, 0, 2]}\n",
    "\n",
    "docs = []\n",
    "for tag, sent in data_sents.items():\n",
    "    words = [str(w) for w in tokenizer.tokenize(text=sent)]\n",
    "    labels = data_labels[tag]\n",
    "    docs.append(NER_LabeledSentence(tag=tag, words=words, labels=labels))\n",
    "\n",
    "max_sent_len = 10\n",
    "ner_corpus = NER_Corpus(docs=docs, ner_labels=ner_labels, max_sent_len=max_sent_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from connlp.preprocess import EnglishTokenizer\n",
    "from connlp.embedding import Vectorizer\n",
    "tokenizer = EnglishTokenizer()\n",
    "vectorizer = Vectorizer()\n",
    "\n",
    "tokenized_sents = [tokenizer.tokenize(sent) for sent in data_sents.values()]\n",
    "w2v_model = vectorizer.word2vec(docs=tokenized_sents)\n",
    "\n",
    "word2vector = vectorizer.get_word_vectors(w2v_model)\n",
    "feature_size = w2v_model.vector_size\n",
    "ner_corpus.word_embedding(word2vector=word2vector, feature_size=feature_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from connlp.analysis import NER_Model\n",
    "\n",
    "parameters = {\n",
    "    # Parameters for Bi-LSTM.\n",
    "    'lstm_units': 512,\n",
    "    'lstm_return_sequences': True,\n",
    "    'lstm_recurrent_dropout': 0.2,\n",
    "    'dense_units': 100,\n",
    "    'dense_activation': 'relu',\n",
    "\n",
    "    # Parameters for model training.\n",
    "    'test_size': 0.3,\n",
    "    'batch_size': 1,\n",
    "    'epochs': 100,\n",
    "    'validation_split': 0.1,\n",
    "}\n",
    "\n",
    "ner_model = NER_Model()\n",
    "ner_model.initialize(ner_corpus=ner_corpus, parameters=parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3 samples, validate on 1 samples\n",
      "Epoch 1/100\n"
     ]
    }
   ],
   "source": [
    "ner_model.train(parameters=parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--------------------------------------------------\n",
      "|Confusion Matrix:\n",
      "[[ 2  1  1  4]\n",
      " [ 0  3  1  4]\n",
      " [ 1  0  1  2]\n",
      " [ 3  4  3 10]]\n",
      "|--------------------------------------------------\n",
      "|F1 Score: 0.680\n",
      "|--------------------------------------------------\n",
      "|    [NON]: 0.571\n",
      "|    [PER]: 0.750\n",
      "|    [FOD]: 0.400\n"
     ]
    }
   ],
   "source": [
    "ner_model.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Save & Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from connlp.util import makedir\n",
    "\n",
    "fpath_model = 'test/ner/model.pk'\n",
    "makedir(fpath=fpath_model)\n",
    "ner_model.save(fpath_model=fpath_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath_model = 'test/ner/model.pk'\n",
    "ner_model = NER_Model()\n",
    "ner_model.load(fpath_model=fpath_model, ner_corpus=ner_corpus, parameters=parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tom/PER eats/NON apple/FOD\n"
     ]
    }
   ],
   "source": [
    "from connlp.preprocess import EnglishTokenizer\n",
    "vectorizer = Vectorizer()\n",
    "\n",
    "new_sent = 'Tom eats apple'\n",
    "tokenized_sent = tokenizer.tokenize(new_sent)\n",
    "ner_result = ner_model.predict(sent=tokenized_sent)\n",
    "print(ner_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
