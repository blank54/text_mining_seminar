{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.system('pip install --upgrade gensim') # if Gensim is not installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/blank54/anaconda3/envs/study/lib/python3.6/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0.0\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "import json\n",
    "from glob import glob\n",
    "import logging\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pprint import pprint # pretty print | https://docs.python.org/ko/3/library/pprint.html\n",
    "\n",
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /data/blank54/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_corpus(corpus_dir):\n",
    "    fpaths = glob(corpus_dir + '/*')\n",
    "    corpus = []\n",
    "    for path in fpaths:\n",
    "        with open(path, 'r') as f:\n",
    "            doc = json.load(f)\n",
    "            content = doc['content']\n",
    "            doc_text = word_tokenize(content)\n",
    "            corpus.append(doc_text)\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" It will take a minute \"\"\"\n",
    "corpus_dir = '/data/sech/workspace/text_mining_seminar/20210330_word2vec/WorldBankNews/'\n",
    "corpus = load_json_corpus(corpus_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 9169\n"
     ]
    }
   ],
   "source": [
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Word2vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-30 14:19:56,140 : INFO : collecting all words and their counts\n",
      "2021-03-30 14:19:56,141 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-03-30 14:19:57,368 : INFO : collected 106683 word types from a corpus of 8194685 raw words and 9169 sentences\n",
      "2021-03-30 14:19:57,369 : INFO : Creating a fresh vocabulary\n",
      "2021-03-30 14:19:57,501 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 29017 unique words (27.19927261138138%% of original 106683, drops 77666)', 'datetime': '2021-03-30T14:19:57.499799', 'gensim': '4.0.0', 'python': '3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-107-generic-x86_64-with-debian-stretch-sid', 'event': 'prepare_vocab'}\n",
      "2021-03-30 14:19:57,501 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 8073911 word corpus (98.52619106164545%% of original 8194685, drops 120774)', 'datetime': '2021-03-30T14:19:57.501889', 'gensim': '4.0.0', 'python': '3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-107-generic-x86_64-with-debian-stretch-sid', 'event': 'prepare_vocab'}\n",
      "2021-03-30 14:19:57,653 : INFO : deleting the raw counts dictionary of 106683 items\n",
      "2021-03-30 14:19:57,657 : INFO : sample=0.001 downsamples 43 most-common words\n",
      "2021-03-30 14:19:57,658 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 5960209.585695938 word corpus (73.8%% of prior 8073911)', 'datetime': '2021-03-30T14:19:57.658238', 'gensim': '4.0.0', 'python': '3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-107-generic-x86_64-with-debian-stretch-sid', 'event': 'prepare_vocab'}\n",
      "2021-03-30 14:19:57,919 : INFO : estimated required memory for 29017 words and 100 dimensions: 37722100 bytes\n",
      "2021-03-30 14:19:57,920 : INFO : resetting layer weights\n",
      "2021-03-30 14:19:57,931 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2021-03-30T14:19:57.931835', 'gensim': '4.0.0', 'python': '3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-107-generic-x86_64-with-debian-stretch-sid', 'event': 'build_vocab'}\n",
      "2021-03-30 14:19:57,932 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 29017 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5', 'datetime': '2021-03-30T14:19:57.932811', 'gensim': '4.0.0', 'python': '3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-107-generic-x86_64-with-debian-stretch-sid', 'event': 'train'}\n",
      "2021-03-30 14:19:58,940 : INFO : EPOCH 1 - PROGRESS: at 24.64% examples, 1434520 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-30 14:19:59,940 : INFO : EPOCH 1 - PROGRESS: at 47.45% examples, 1405891 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-30 14:20:00,944 : INFO : EPOCH 1 - PROGRESS: at 66.80% examples, 1328871 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-30 14:20:01,955 : INFO : EPOCH 1 - PROGRESS: at 86.49% examples, 1284593 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-30 14:20:02,635 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-30 14:20:02,637 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-30 14:20:02,644 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-30 14:20:02,645 : INFO : EPOCH - 1 : training on 8194685 raw words (5957748 effective words) took 4.7s, 1265138 effective words/s\n",
      "2021-03-30 14:20:03,649 : INFO : EPOCH 2 - PROGRESS: at 25.07% examples, 1474559 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-30 14:20:04,658 : INFO : EPOCH 2 - PROGRESS: at 50.35% examples, 1491121 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-30 14:20:05,659 : INFO : EPOCH 2 - PROGRESS: at 74.80% examples, 1486521 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-30 14:20:06,665 : INFO : EPOCH 2 - PROGRESS: at 96.90% examples, 1437673 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-30 14:20:06,805 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-30 14:20:06,809 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-30 14:20:06,811 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-30 14:20:06,811 : INFO : EPOCH - 2 : training on 8194685 raw words (5959109 effective words) took 4.2s, 1431439 effective words/s\n",
      "2021-03-30 14:20:07,824 : INFO : EPOCH 3 - PROGRESS: at 19.25% examples, 1102860 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-30 14:20:08,825 : INFO : EPOCH 3 - PROGRESS: at 38.49% examples, 1135746 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-30 14:20:09,828 : INFO : EPOCH 3 - PROGRESS: at 57.73% examples, 1141813 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-30 14:20:10,833 : INFO : EPOCH 3 - PROGRESS: at 77.05% examples, 1147790 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-30 14:20:11,842 : INFO : EPOCH 3 - PROGRESS: at 96.90% examples, 1148648 words/s, in_qsize 4, out_qsize 1\n",
      "2021-03-30 14:20:11,987 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-30 14:20:11,989 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-30 14:20:11,990 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-30 14:20:11,991 : INFO : EPOCH - 3 : training on 8194685 raw words (5958328 effective words) took 5.2s, 1151123 effective words/s\n",
      "2021-03-30 14:20:12,998 : INFO : EPOCH 4 - PROGRESS: at 19.74% examples, 1143105 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-30 14:20:14,000 : INFO : EPOCH 4 - PROGRESS: at 42.55% examples, 1265666 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-30 14:20:15,006 : INFO : EPOCH 4 - PROGRESS: at 67.95% examples, 1352320 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-30 14:20:16,008 : INFO : EPOCH 4 - PROGRESS: at 94.19% examples, 1398651 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-30 14:20:16,229 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-30 14:20:16,232 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-30 14:20:16,235 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-30 14:20:16,236 : INFO : EPOCH - 4 : training on 8194685 raw words (5959034 effective words) took 4.2s, 1404814 effective words/s\n",
      "2021-03-30 14:20:17,243 : INFO : EPOCH 5 - PROGRESS: at 19.37% examples, 1115981 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-30 14:20:18,244 : INFO : EPOCH 5 - PROGRESS: at 38.95% examples, 1153294 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-30 14:20:19,250 : INFO : EPOCH 5 - PROGRESS: at 58.23% examples, 1152040 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-30 14:20:20,251 : INFO : EPOCH 5 - PROGRESS: at 77.23% examples, 1152736 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-30 14:20:21,255 : INFO : EPOCH 5 - PROGRESS: at 96.90% examples, 1151120 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-30 14:20:21,401 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-30 14:20:21,404 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-30 14:20:21,411 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-30 14:20:21,412 : INFO : EPOCH - 5 : training on 8194685 raw words (5958271 effective words) took 5.2s, 1151828 effective words/s\n",
      "2021-03-30 14:20:21,413 : INFO : Word2Vec lifecycle event {'msg': 'training on 40973425 raw words (29792490 effective words) took 23.5s, 1268854 effective words/s', 'datetime': '2021-03-30T14:20:21.413469', 'gensim': '4.0.0', 'python': '3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-107-generic-x86_64-with-debian-stretch-sid', 'event': 'train'}\n",
      "2021-03-30 14:20:21,414 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec(vocab=29017, vector_size=100, alpha=0.025)', 'datetime': '2021-03-30T14:20:21.414496', 'gensim': '4.0.0', 'python': '3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-107-generic-x86_64-with-debian-stretch-sid', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "\"\"\" It will take a minute \"\"\"\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "wb_w2v = Word2Vec(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.word2vec.Word2Vec'>\n"
     ]
    }
   ],
   "source": [
    "print(type(wb_w2v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_w2v.wv.most_similar('bank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get word vector\n",
    "print('word vector for \"bank\": (size: %d)' % len(wb_w2v.wv['bank']))\n",
    "pprint(wb_w2v.wv['bank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine distance b/w \"world\" and \"bank\"\n",
      "0.8492620140314102\n",
      "\n",
      "cosine similarity b/w \"world\" and \"bank\"\n",
      "0.15073798596858978\n"
     ]
    }
   ],
   "source": [
    "# Similarity b/w 2 words\n",
    "from scipy.spatial.distance import cosine\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html\n",
    "\n",
    "query1 = 'world'\n",
    "query2 = 'bank'\n",
    "wv1 = wb_w2v.wv[query1]\n",
    "wv2 = wb_w2v.wv[query2]\n",
    "print('cosine distance b/w \"%s\" and \"%s\"' % (query1, query2))\n",
    "print(cosine(wv1, wv2))\n",
    "print()\n",
    "print('cosine similarity b/w \"%s\" and \"%s\"' % (query1, query2))\n",
    "print(1 - cosine(wv1, wv2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most similar words\n",
    "query1 = 'world'\n",
    "query2 = 'bank'\n",
    "print('Most similar words with \"%s\"' % query1)\n",
    "pprint(wb_w2v.wv.most_similar('world'))\n",
    "print()\n",
    "print('Most 5 similar words with \"%s\"' % query2)\n",
    "pprint(wb_w2v.wv.most_similar('bank', topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additive Composition\n",
    "\"\"\"\n",
    "Usage:\n",
    "Positive word 1 - Negative word 1 + Positive word 2 = Result\n",
    "Same as Pos1 : Neg1 = Result : Pos2\n",
    "(e.g. \"Korea\" - \"Seoul\" + \"Tokyo\" = ? ; i.e. Korea:Seoul = ?:Tokyo)\n",
    "\"\"\"\n",
    "\n",
    "pos1 = 'Korea'\n",
    "neg1 = 'Seoul'\n",
    "pos2 = 'Tokyo'\n",
    "# pos1 : neg1 = (result) : pos2\n",
    "k = 5\n",
    "print('%d candidate words for the nation whose capital city is %s:' % (k, pos2))\n",
    "pprint(wb_w2v.wv.most_similar(positive=[pos1, pos2], negative=[neg1], topn=k)) # Expecting \"Japan\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained Word2vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-30 14:30:20,581 : INFO : Creating /data/blank54/gensim-data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'corpora': {'20-newsgroups': {'checksum': 'c92fd4f6640a86d5ba89eaad818a9891',\n",
      "                               'description': 'The notorious collection of '\n",
      "                                              'approximately 20,000 newsgroup '\n",
      "                                              'posts, partitioned (nearly) '\n",
      "                                              'evenly across 20 different '\n",
      "                                              'newsgroups.',\n",
      "                               'fields': {'data': '',\n",
      "                                          'id': 'original id inferred from '\n",
      "                                                'folder name',\n",
      "                                          'set': 'marker of original split '\n",
      "                                                 \"(possible values 'train' and \"\n",
      "                                                 \"'test')\",\n",
      "                                          'topic': 'name of topic (20 variant '\n",
      "                                                   'of possible values)'},\n",
      "                               'file_name': '20-newsgroups.gz',\n",
      "                               'file_size': 14483581,\n",
      "                               'license': 'not found',\n",
      "                               'num_records': 18846,\n",
      "                               'parts': 1,\n",
      "                               'read_more': ['http://qwone.com/~jason/20Newsgroups/'],\n",
      "                               'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/20-newsgroups/__init__.py',\n",
      "                               'record_format': 'dict'},\n",
      "             '__testing_matrix-synopsis': {'checksum': '1767ac93a089b43899d54944b07d9dc5',\n",
      "                                           'description': '[THIS IS ONLY FOR '\n",
      "                                                          'TESTING] Synopsis '\n",
      "                                                          'of the movie '\n",
      "                                                          'matrix.',\n",
      "                                           'file_name': '__testing_matrix-synopsis.gz',\n",
      "                                           'parts': 1,\n",
      "                                           'read_more': ['http://www.imdb.com/title/tt0133093/plotsummary?ref_=ttpl_pl_syn#synopsis']},\n",
      "             '__testing_multipart-matrix-synopsis': {'checksum-0': 'c8b0c7d8cf562b1b632c262a173ac338',\n",
      "                                                     'checksum-1': '5ff7fc6818e9a5d9bc1cf12c35ed8b96',\n",
      "                                                     'checksum-2': '966db9d274d125beaac7987202076cba',\n",
      "                                                     'description': '[THIS IS '\n",
      "                                                                    'ONLY FOR '\n",
      "                                                                    'TESTING] '\n",
      "                                                                    'Synopsis '\n",
      "                                                                    'of the '\n",
      "                                                                    'movie '\n",
      "                                                                    'matrix.',\n",
      "                                                     'file_name': '__testing_multipart-matrix-synopsis.gz',\n",
      "                                                     'parts': 3,\n",
      "                                                     'read_more': ['http://www.imdb.com/title/tt0133093/plotsummary?ref_=ttpl_pl_syn#synopsis']},\n",
      "             'fake-news': {'checksum': '5e64e942df13219465927f92dcefd5fe',\n",
      "                           'description': 'News dataset, contains text and '\n",
      "                                          'metadata from 244 websites and '\n",
      "                                          'represents 12,999 posts in total '\n",
      "                                          'from a specific window of 30 days. '\n",
      "                                          'The data was pulled using the '\n",
      "                                          \"webhose.io API, and because it's \"\n",
      "                                          'coming from their crawler, not all '\n",
      "                                          'websites identified by their BS '\n",
      "                                          'Detector are present in this '\n",
      "                                          'dataset. Data sources that were '\n",
      "                                          'missing a label were simply '\n",
      "                                          \"assigned a label of 'bs'. There are \"\n",
      "                                          '(ostensibly) no genuine, reliable, '\n",
      "                                          'or trustworthy news sources '\n",
      "                                          'represented in this dataset (so '\n",
      "                                          \"far), so don't trust anything you \"\n",
      "                                          'read.',\n",
      "                           'fields': {'author': 'author of story',\n",
      "                                      'comments': 'number of Facebook comments',\n",
      "                                      'country': 'data from webhose.io',\n",
      "                                      'crawled': 'date the story was archived',\n",
      "                                      'domain_rank': 'data from webhose.io',\n",
      "                                      'language': 'data from webhose.io',\n",
      "                                      'likes': 'number of Facebook likes',\n",
      "                                      'main_img_url': 'image from story',\n",
      "                                      'ord_in_thread': '',\n",
      "                                      'participants_count': 'number of '\n",
      "                                                            'participants',\n",
      "                                      'published': 'date published',\n",
      "                                      'replies_count': 'number of replies',\n",
      "                                      'shares': 'number of Facebook shares',\n",
      "                                      'site_url': 'site URL from BS detector',\n",
      "                                      'spam_score': 'data from webhose.io',\n",
      "                                      'text': 'text of story',\n",
      "                                      'thread_title': '',\n",
      "                                      'title': 'title of story',\n",
      "                                      'type': 'type of website (label from BS '\n",
      "                                              'detector)',\n",
      "                                      'uuid': 'unique identifier'},\n",
      "                           'file_name': 'fake-news.gz',\n",
      "                           'file_size': 20102776,\n",
      "                           'license': 'https://creativecommons.org/publicdomain/zero/1.0/',\n",
      "                           'num_records': 12999,\n",
      "                           'parts': 1,\n",
      "                           'read_more': ['https://www.kaggle.com/mrisdal/fake-news'],\n",
      "                           'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/fake-news/__init__.py',\n",
      "                           'record_format': 'dict'},\n",
      "             'patent-2017': {'checksum-0': '818501f0b9af62d3b88294d86d509f8f',\n",
      "                             'checksum-1': '66c05635c1d3c7a19b4a335829d09ffa',\n",
      "                             'description': 'Patent Grant Full Text. Contains '\n",
      "                                            'the full text including tables, '\n",
      "                                            \"sequence data and 'in-line' \"\n",
      "                                            'mathematical expressions of each '\n",
      "                                            'patent grant issued in 2017.',\n",
      "                             'file_name': 'patent-2017.gz',\n",
      "                             'file_size': 3087262469,\n",
      "                             'license': 'not found',\n",
      "                             'num_records': 353197,\n",
      "                             'parts': 2,\n",
      "                             'read_more': ['http://patents.reedtech.com/pgrbft.php'],\n",
      "                             'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/patent-2017/__init__.py',\n",
      "                             'record_format': 'dict'},\n",
      "             'quora-duplicate-questions': {'checksum': 'd7cfa7fbc6e2ec71ab74c495586c6365',\n",
      "                                           'description': 'Over 400,000 lines '\n",
      "                                                          'of potential '\n",
      "                                                          'question duplicate '\n",
      "                                                          'pairs. Each line '\n",
      "                                                          'contains IDs for '\n",
      "                                                          'each question in '\n",
      "                                                          'the pair, the full '\n",
      "                                                          'text for each '\n",
      "                                                          'question, and a '\n",
      "                                                          'binary value that '\n",
      "                                                          'indicates whether '\n",
      "                                                          'the line contains a '\n",
      "                                                          'duplicate pair or '\n",
      "                                                          'not.',\n",
      "                                           'fields': {'id': 'the id of a '\n",
      "                                                            'training set '\n",
      "                                                            'question pair',\n",
      "                                                      'is_duplicate': 'the '\n",
      "                                                                      'target '\n",
      "                                                                      'variable, '\n",
      "                                                                      'set to '\n",
      "                                                                      '1 if '\n",
      "                                                                      'question1 '\n",
      "                                                                      'and '\n",
      "                                                                      'question2 '\n",
      "                                                                      'have '\n",
      "                                                                      'essentially '\n",
      "                                                                      'the '\n",
      "                                                                      'same '\n",
      "                                                                      'meaning, '\n",
      "                                                                      'and 0 '\n",
      "                                                                      'otherwise',\n",
      "                                                      'qid1': 'unique ids of '\n",
      "                                                              'each question',\n",
      "                                                      'qid2': 'unique ids of '\n",
      "                                                              'each question',\n",
      "                                                      'question1': 'the full '\n",
      "                                                                   'text of '\n",
      "                                                                   'each '\n",
      "                                                                   'question',\n",
      "                                                      'question2': 'the full '\n",
      "                                                                   'text of '\n",
      "                                                                   'each '\n",
      "                                                                   'question'},\n",
      "                                           'file_name': 'quora-duplicate-questions.gz',\n",
      "                                           'file_size': 21684784,\n",
      "                                           'license': 'probably '\n",
      "                                                      'https://www.quora.com/about/tos',\n",
      "                                           'num_records': 404290,\n",
      "                                           'parts': 1,\n",
      "                                           'read_more': ['https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs'],\n",
      "                                           'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/quora-duplicate-questions/__init__.py',\n",
      "                                           'record_format': 'dict'},\n",
      "             'semeval-2016-2017-task3-subtaskA-unannotated': {'checksum': '2de0e2f2c4f91c66ae4fcf58d50ba816',\n",
      "                                                              'description': 'SemEval '\n",
      "                                                                             '2016 '\n",
      "                                                                             '/ '\n",
      "                                                                             '2017 '\n",
      "                                                                             'Task '\n",
      "                                                                             '3 '\n",
      "                                                                             'Subtask '\n",
      "                                                                             'A '\n",
      "                                                                             'unannotated '\n",
      "                                                                             'dataset '\n",
      "                                                                             'contains '\n",
      "                                                                             '189,941 '\n",
      "                                                                             'questions '\n",
      "                                                                             'and '\n",
      "                                                                             '1,894,456 '\n",
      "                                                                             'comments '\n",
      "                                                                             'in '\n",
      "                                                                             'English '\n",
      "                                                                             'collected '\n",
      "                                                                             'from '\n",
      "                                                                             'the '\n",
      "                                                                             'Community '\n",
      "                                                                             'Question '\n",
      "                                                                             'Answering '\n",
      "                                                                             '(CQA) '\n",
      "                                                                             'web '\n",
      "                                                                             'forum '\n",
      "                                                                             'of '\n",
      "                                                                             'Qatar '\n",
      "                                                                             'Living. '\n",
      "                                                                             'These '\n",
      "                                                                             'can '\n",
      "                                                                             'be '\n",
      "                                                                             'used '\n",
      "                                                                             'as '\n",
      "                                                                             'a '\n",
      "                                                                             'corpus '\n",
      "                                                                             'for '\n",
      "                                                                             'language '\n",
      "                                                                             'modelling.',\n",
      "                                                              'fields': {'RelComments': [{'RELC_DATE': 'date '\n",
      "                                                                                                       'of '\n",
      "                                                                                                       'posting',\n",
      "                                                                                          'RELC_ID': 'comment '\n",
      "                                                                                                     'identifier',\n",
      "                                                                                          'RELC_USERID': 'identifier '\n",
      "                                                                                                         'of '\n",
      "                                                                                                         'the '\n",
      "                                                                                                         'user '\n",
      "                                                                                                         'posting '\n",
      "                                                                                                         'the '\n",
      "                                                                                                         'comment',\n",
      "                                                                                          'RELC_USERNAME': 'name '\n",
      "                                                                                                           'of '\n",
      "                                                                                                           'the '\n",
      "                                                                                                           'user '\n",
      "                                                                                                           'posting '\n",
      "                                                                                                           'the '\n",
      "                                                                                                           'comment',\n",
      "                                                                                          'RelCText': 'text '\n",
      "                                                                                                      'of '\n",
      "                                                                                                      'answer'}],\n",
      "                                                                         'RelQuestion': {'RELQ_CATEGORY': 'question '\n",
      "                                                                                                          'category, '\n",
      "                                                                                                          'according '\n",
      "                                                                                                          'to '\n",
      "                                                                                                          'the '\n",
      "                                                                                                          'Qatar '\n",
      "                                                                                                          'Living '\n",
      "                                                                                                          'taxonomy',\n",
      "                                                                                         'RELQ_DATE': 'date '\n",
      "                                                                                                      'of '\n",
      "                                                                                                      'posting',\n",
      "                                                                                         'RELQ_ID': 'question '\n",
      "                                                                                                    'indentifier',\n",
      "                                                                                         'RELQ_USERID': 'identifier '\n",
      "                                                                                                        'of '\n",
      "                                                                                                        'the '\n",
      "                                                                                                        'user '\n",
      "                                                                                                        'asking '\n",
      "                                                                                                        'the '\n",
      "                                                                                                        'question',\n",
      "                                                                                         'RELQ_USERNAME': 'name '\n",
      "                                                                                                          'of '\n",
      "                                                                                                          'the '\n",
      "                                                                                                          'user '\n",
      "                                                                                                          'asking '\n",
      "                                                                                                          'the '\n",
      "                                                                                                          'question',\n",
      "                                                                                         'RelQBody': 'body '\n",
      "                                                                                                     'of '\n",
      "                                                                                                     'question',\n",
      "                                                                                         'RelQSubject': 'subject '\n",
      "                                                                                                        'of '\n",
      "                                                                                                        'question'},\n",
      "                                                                         'THREAD_SEQUENCE': ''},\n",
      "                                                              'file_name': 'semeval-2016-2017-task3-subtaskA-unannotated.gz',\n",
      "                                                              'file_size': 234373151,\n",
      "                                                              'license': 'These '\n",
      "                                                                         'datasets '\n",
      "                                                                         'are '\n",
      "                                                                         'free '\n",
      "                                                                         'for '\n",
      "                                                                         'general '\n",
      "                                                                         'research '\n",
      "                                                                         'use.',\n",
      "                                                              'num_records': 189941,\n",
      "                                                              'parts': 1,\n",
      "                                                              'read_more': ['http://alt.qcri.org/semeval2016/task3/',\n",
      "                                                                            'http://alt.qcri.org/semeval2016/task3/data/uploads/semeval2016-task3-report.pdf',\n",
      "                                                                            'https://github.com/RaRe-Technologies/gensim-data/issues/18',\n",
      "                                                                            'https://github.com/Witiko/semeval-2016_2017-task3-subtaskA-unannotated-english'],\n",
      "                                                              'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/semeval-2016-2017-task3-subtaskA-unannotated-eng/__init__.py',\n",
      "                                                              'record_format': 'dict'},\n",
      "             'semeval-2016-2017-task3-subtaskBC': {'checksum': '701ea67acd82e75f95e1d8e62fb0ad29',\n",
      "                                                   'description': 'SemEval '\n",
      "                                                                  '2016 / 2017 '\n",
      "                                                                  'Task 3 '\n",
      "                                                                  'Subtask B '\n",
      "                                                                  'and C '\n",
      "                                                                  'datasets '\n",
      "                                                                  'contain '\n",
      "                                                                  'train+development '\n",
      "                                                                  '(317 '\n",
      "                                                                  'original '\n",
      "                                                                  'questions, '\n",
      "                                                                  '3,169 '\n",
      "                                                                  'related '\n",
      "                                                                  'questions, '\n",
      "                                                                  'and 31,690 '\n",
      "                                                                  'comments), '\n",
      "                                                                  'and test '\n",
      "                                                                  'datasets in '\n",
      "                                                                  'English. '\n",
      "                                                                  'The '\n",
      "                                                                  'description '\n",
      "                                                                  'of the '\n",
      "                                                                  'tasks and '\n",
      "                                                                  'the '\n",
      "                                                                  'collected '\n",
      "                                                                  'data is '\n",
      "                                                                  'given in '\n",
      "                                                                  'sections 3 '\n",
      "                                                                  'and 4.1 of '\n",
      "                                                                  'the task '\n",
      "                                                                  'paper '\n",
      "                                                                  'http://alt.qcri.org/semeval2016/task3/data/uploads/semeval2016-task3-report.pdf '\n",
      "                                                                  'linked in '\n",
      "                                                                  'section '\n",
      "                                                                  '“Papers” of '\n",
      "                                                                  'https://github.com/RaRe-Technologies/gensim-data/issues/18.',\n",
      "                                                   'fields': {'2016-dev': ['...'],\n",
      "                                                              '2016-test': ['...'],\n",
      "                                                              '2016-train': ['...'],\n",
      "                                                              '2017-test': ['...']},\n",
      "                                                   'file_name': 'semeval-2016-2017-task3-subtaskBC.gz',\n",
      "                                                   'file_size': 6344358,\n",
      "                                                   'license': 'All files '\n",
      "                                                              'released for '\n",
      "                                                              'the task are '\n",
      "                                                              'free for '\n",
      "                                                              'general '\n",
      "                                                              'research use',\n",
      "                                                   'num_records': -1,\n",
      "                                                   'parts': 1,\n",
      "                                                   'read_more': ['http://alt.qcri.org/semeval2017/task3/',\n",
      "                                                                 'http://alt.qcri.org/semeval2017/task3/data/uploads/semeval2017-task3.pdf',\n",
      "                                                                 'https://github.com/RaRe-Technologies/gensim-data/issues/18',\n",
      "                                                                 'https://github.com/Witiko/semeval-2016_2017-task3-subtaskB-english'],\n",
      "                                                   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/semeval-2016-2017-task3-subtaskB-eng/__init__.py',\n",
      "                                                   'record_format': 'dict'},\n",
      "             'text8': {'checksum': '68799af40b6bda07dfa47a32612e5364',\n",
      "                       'description': 'First 100,000,000 bytes of plain text '\n",
      "                                      'from Wikipedia. Used for testing '\n",
      "                                      'purposes; see wiki-english-* for proper '\n",
      "                                      'full Wikipedia datasets.',\n",
      "                       'file_name': 'text8.gz',\n",
      "                       'file_size': 33182058,\n",
      "                       'license': 'not found',\n",
      "                       'num_records': 1701,\n",
      "                       'parts': 1,\n",
      "                       'read_more': ['http://mattmahoney.net/dc/textdata.html'],\n",
      "                       'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/text8/__init__.py',\n",
      "                       'record_format': 'list of str (tokens)'},\n",
      "             'wiki-english-20171001': {'checksum-0': 'a7d7d7fd41ea7e2d7fa32ec1bb640d71',\n",
      "                                       'checksum-1': 'b2683e3356ffbca3b6c2dca6e9801f9f',\n",
      "                                       'checksum-2': 'c5cde2a9ae77b3c4ebce804f6df542c2',\n",
      "                                       'checksum-3': '00b71144ed5e3aeeb885de84f7452b81',\n",
      "                                       'description': 'Extracted Wikipedia '\n",
      "                                                      'dump from October 2017. '\n",
      "                                                      'Produced by `python -m '\n",
      "                                                      'gensim.scripts.segment_wiki '\n",
      "                                                      '-f '\n",
      "                                                      'enwiki-20171001-pages-articles.xml.bz2 '\n",
      "                                                      '-o wiki-en.gz`',\n",
      "                                       'fields': {'section_texts': 'list of '\n",
      "                                                                   'body of '\n",
      "                                                                   'sections',\n",
      "                                                  'section_titles': 'list of '\n",
      "                                                                    'titles of '\n",
      "                                                                    'sections',\n",
      "                                                  'title': 'Title of wiki '\n",
      "                                                           'article'},\n",
      "                                       'file_name': 'wiki-english-20171001.gz',\n",
      "                                       'file_size': 6516051717,\n",
      "                                       'license': 'https://dumps.wikimedia.org/legal.html',\n",
      "                                       'num_records': 4924894,\n",
      "                                       'parts': 4,\n",
      "                                       'read_more': ['https://dumps.wikimedia.org/enwiki/20171001/'],\n",
      "                                       'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/wiki-english-20171001/__init__.py',\n",
      "                                       'record_format': 'dict'}},\n",
      " 'models': {'__testing_word2vec-matrix-synopsis': {'checksum': '534dcb8b56a360977a269b7bfc62d124',\n",
      "                                                   'description': '[THIS IS '\n",
      "                                                                  'ONLY FOR '\n",
      "                                                                  'TESTING] '\n",
      "                                                                  'Word '\n",
      "                                                                  'vecrors of '\n",
      "                                                                  'the movie '\n",
      "                                                                  'matrix.',\n",
      "                                                   'file_name': '__testing_word2vec-matrix-synopsis.gz',\n",
      "                                                   'parameters': {'dimensions': 50},\n",
      "                                                   'parts': 1,\n",
      "                                                   'preprocessing': 'Converted '\n",
      "                                                                    'to w2v '\n",
      "                                                                    'using a '\n",
      "                                                                    'preprocessed '\n",
      "                                                                    'corpus. '\n",
      "                                                                    'Converted '\n",
      "                                                                    'to w2v '\n",
      "                                                                    'format '\n",
      "                                                                    'with '\n",
      "                                                                    '`python3.5 '\n",
      "                                                                    '-m '\n",
      "                                                                    'gensim.models.word2vec '\n",
      "                                                                    '-train '\n",
      "                                                                    '<input_filename> '\n",
      "                                                                    '-iter 50 '\n",
      "                                                                    '-output '\n",
      "                                                                    '<output_filename>`.',\n",
      "                                                   'read_more': []},\n",
      "            'conceptnet-numberbatch-17-06-300': {'base_dataset': 'ConceptNet, '\n",
      "                                                                 'word2vec, '\n",
      "                                                                 'GloVe, and '\n",
      "                                                                 'OpenSubtitles '\n",
      "                                                                 '2016',\n",
      "                                                 'checksum': 'fd642d457adcd0ea94da0cd21b150847',\n",
      "                                                 'description': 'ConceptNet '\n",
      "                                                                'Numberbatch '\n",
      "                                                                'consists of '\n",
      "                                                                'state-of-the-art '\n",
      "                                                                'semantic '\n",
      "                                                                'vectors (also '\n",
      "                                                                'known as word '\n",
      "                                                                'embeddings) '\n",
      "                                                                'that can be '\n",
      "                                                                'used directly '\n",
      "                                                                'as a '\n",
      "                                                                'representation '\n",
      "                                                                'of word '\n",
      "                                                                'meanings or '\n",
      "                                                                'as a starting '\n",
      "                                                                'point for '\n",
      "                                                                'further '\n",
      "                                                                'machine '\n",
      "                                                                'learning. '\n",
      "                                                                'ConceptNet '\n",
      "                                                                'Numberbatch '\n",
      "                                                                'is part of '\n",
      "                                                                'the '\n",
      "                                                                'ConceptNet '\n",
      "                                                                'open data '\n",
      "                                                                'project. '\n",
      "                                                                'ConceptNet '\n",
      "                                                                'provides lots '\n",
      "                                                                'of ways to '\n",
      "                                                                'compute with '\n",
      "                                                                'word '\n",
      "                                                                'meanings, one '\n",
      "                                                                'of which is '\n",
      "                                                                'word '\n",
      "                                                                'embeddings. '\n",
      "                                                                'ConceptNet '\n",
      "                                                                'Numberbatch '\n",
      "                                                                'is a snapshot '\n",
      "                                                                'of just the '\n",
      "                                                                'word '\n",
      "                                                                'embeddings. '\n",
      "                                                                'It is built '\n",
      "                                                                'using an '\n",
      "                                                                'ensemble that '\n",
      "                                                                'combines data '\n",
      "                                                                'from '\n",
      "                                                                'ConceptNet, '\n",
      "                                                                'word2vec, '\n",
      "                                                                'GloVe, and '\n",
      "                                                                'OpenSubtitles '\n",
      "                                                                '2016, using a '\n",
      "                                                                'variation on '\n",
      "                                                                'retrofitting.',\n",
      "                                                 'file_name': 'conceptnet-numberbatch-17-06-300.gz',\n",
      "                                                 'file_size': 1225497562,\n",
      "                                                 'license': 'https://github.com/commonsense/conceptnet-numberbatch/blob/master/LICENSE.txt',\n",
      "                                                 'num_records': 1917247,\n",
      "                                                 'parameters': {'dimension': 300},\n",
      "                                                 'parts': 1,\n",
      "                                                 'read_more': ['http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14972',\n",
      "                                                               'https://github.com/commonsense/conceptnet-numberbatch',\n",
      "                                                               'http://conceptnet.io/'],\n",
      "                                                 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/conceptnet-numberbatch-17-06-300/__init__.py'},\n",
      "            'fasttext-wiki-news-subwords-300': {'base_dataset': 'Wikipedia '\n",
      "                                                                '2017, UMBC '\n",
      "                                                                'webbase '\n",
      "                                                                'corpus and '\n",
      "                                                                'statmt.org '\n",
      "                                                                'news dataset '\n",
      "                                                                '(16B tokens)',\n",
      "                                                'checksum': 'de2bb3a20c46ce65c9c131e1ad9a77af',\n",
      "                                                'description': '1 million word '\n",
      "                                                               'vectors '\n",
      "                                                               'trained on '\n",
      "                                                               'Wikipedia '\n",
      "                                                               '2017, UMBC '\n",
      "                                                               'webbase corpus '\n",
      "                                                               'and statmt.org '\n",
      "                                                               'news dataset '\n",
      "                                                               '(16B tokens).',\n",
      "                                                'file_name': 'fasttext-wiki-news-subwords-300.gz',\n",
      "                                                'file_size': 1005007116,\n",
      "                                                'license': 'https://creativecommons.org/licenses/by-sa/3.0/',\n",
      "                                                'num_records': 999999,\n",
      "                                                'parameters': {'dimension': 300},\n",
      "                                                'parts': 1,\n",
      "                                                'read_more': ['https://fasttext.cc/docs/en/english-vectors.html',\n",
      "                                                              'https://arxiv.org/abs/1712.09405',\n",
      "                                                              'https://arxiv.org/abs/1607.01759'],\n",
      "                                                'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/fasttext-wiki-news-subwords-300/__init__.py'},\n",
      "            'glove-twitter-100': {'base_dataset': 'Twitter (2B tweets, 27B '\n",
      "                                                  'tokens, 1.2M vocab, '\n",
      "                                                  'uncased)',\n",
      "                                  'checksum': 'b04f7bed38756d64cf55b58ce7e97b15',\n",
      "                                  'description': 'Pre-trained vectors based '\n",
      "                                                 'on  2B tweets, 27B tokens, '\n",
      "                                                 '1.2M vocab, uncased '\n",
      "                                                 '(https://nlp.stanford.edu/projects/glove/)',\n",
      "                                  'file_name': 'glove-twitter-100.gz',\n",
      "                                  'file_size': 405932991,\n",
      "                                  'license': 'http://opendatacommons.org/licenses/pddl/',\n",
      "                                  'num_records': 1193514,\n",
      "                                  'parameters': {'dimension': 100},\n",
      "                                  'parts': 1,\n",
      "                                  'preprocessing': 'Converted to w2v format '\n",
      "                                                   'with `python -m '\n",
      "                                                   'gensim.scripts.glove2word2vec '\n",
      "                                                   '-i <fname> -o '\n",
      "                                                   'glove-twitter-100.txt`.',\n",
      "                                  'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
      "                                                'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
      "                                  'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-100/__init__.py'},\n",
      "            'glove-twitter-200': {'base_dataset': 'Twitter (2B tweets, 27B '\n",
      "                                                  'tokens, 1.2M vocab, '\n",
      "                                                  'uncased)',\n",
      "                                  'checksum': 'e52e8392d1860b95d5308a525817d8f9',\n",
      "                                  'description': 'Pre-trained vectors based on '\n",
      "                                                 '2B tweets, 27B tokens, 1.2M '\n",
      "                                                 'vocab, uncased '\n",
      "                                                 '(https://nlp.stanford.edu/projects/glove/).',\n",
      "                                  'file_name': 'glove-twitter-200.gz',\n",
      "                                  'file_size': 795373100,\n",
      "                                  'license': 'http://opendatacommons.org/licenses/pddl/',\n",
      "                                  'num_records': 1193514,\n",
      "                                  'parameters': {'dimension': 200},\n",
      "                                  'parts': 1,\n",
      "                                  'preprocessing': 'Converted to w2v format '\n",
      "                                                   'with `python -m '\n",
      "                                                   'gensim.scripts.glove2word2vec '\n",
      "                                                   '-i <fname> -o '\n",
      "                                                   'glove-twitter-200.txt`.',\n",
      "                                  'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
      "                                                'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
      "                                  'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-200/__init__.py'},\n",
      "            'glove-twitter-25': {'base_dataset': 'Twitter (2B tweets, 27B '\n",
      "                                                 'tokens, 1.2M vocab, uncased)',\n",
      "                                 'checksum': '50db0211d7e7a2dcd362c6b774762793',\n",
      "                                 'description': 'Pre-trained vectors based on '\n",
      "                                                '2B tweets, 27B tokens, 1.2M '\n",
      "                                                'vocab, uncased '\n",
      "                                                '(https://nlp.stanford.edu/projects/glove/).',\n",
      "                                 'file_name': 'glove-twitter-25.gz',\n",
      "                                 'file_size': 109885004,\n",
      "                                 'license': 'http://opendatacommons.org/licenses/pddl/',\n",
      "                                 'num_records': 1193514,\n",
      "                                 'parameters': {'dimension': 25},\n",
      "                                 'parts': 1,\n",
      "                                 'preprocessing': 'Converted to w2v format '\n",
      "                                                  'with `python -m '\n",
      "                                                  'gensim.scripts.glove2word2vec '\n",
      "                                                  '-i <fname> -o '\n",
      "                                                  'glove-twitter-25.txt`.',\n",
      "                                 'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
      "                                               'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
      "                                 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-25/__init__.py'},\n",
      "            'glove-twitter-50': {'base_dataset': 'Twitter (2B tweets, 27B '\n",
      "                                                 'tokens, 1.2M vocab, uncased)',\n",
      "                                 'checksum': 'c168f18641f8c8a00fe30984c4799b2b',\n",
      "                                 'description': 'Pre-trained vectors based on '\n",
      "                                                '2B tweets, 27B tokens, 1.2M '\n",
      "                                                'vocab, uncased '\n",
      "                                                '(https://nlp.stanford.edu/projects/glove/)',\n",
      "                                 'file_name': 'glove-twitter-50.gz',\n",
      "                                 'file_size': 209216938,\n",
      "                                 'license': 'http://opendatacommons.org/licenses/pddl/',\n",
      "                                 'num_records': 1193514,\n",
      "                                 'parameters': {'dimension': 50},\n",
      "                                 'parts': 1,\n",
      "                                 'preprocessing': 'Converted to w2v format '\n",
      "                                                  'with `python -m '\n",
      "                                                  'gensim.scripts.glove2word2vec '\n",
      "                                                  '-i <fname> -o '\n",
      "                                                  'glove-twitter-50.txt`.',\n",
      "                                 'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
      "                                               'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
      "                                 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-50/__init__.py'},\n",
      "            'glove-wiki-gigaword-100': {'base_dataset': 'Wikipedia 2014 + '\n",
      "                                                        'Gigaword 5 (6B '\n",
      "                                                        'tokens, uncased)',\n",
      "                                        'checksum': '40ec481866001177b8cd4cb0df92924f',\n",
      "                                        'description': 'Pre-trained vectors '\n",
      "                                                       'based on Wikipedia '\n",
      "                                                       '2014 + Gigaword 5.6B '\n",
      "                                                       'tokens, 400K vocab, '\n",
      "                                                       'uncased '\n",
      "                                                       '(https://nlp.stanford.edu/projects/glove/).',\n",
      "                                        'file_name': 'glove-wiki-gigaword-100.gz',\n",
      "                                        'file_size': 134300434,\n",
      "                                        'license': 'http://opendatacommons.org/licenses/pddl/',\n",
      "                                        'num_records': 400000,\n",
      "                                        'parameters': {'dimension': 100},\n",
      "                                        'parts': 1,\n",
      "                                        'preprocessing': 'Converted to w2v '\n",
      "                                                         'format with `python '\n",
      "                                                         '-m '\n",
      "                                                         'gensim.scripts.glove2word2vec '\n",
      "                                                         '-i <fname> -o '\n",
      "                                                         'glove-wiki-gigaword-100.txt`.',\n",
      "                                        'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
      "                                                      'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
      "                                        'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-100/__init__.py'},\n",
      "            'glove-wiki-gigaword-200': {'base_dataset': 'Wikipedia 2014 + '\n",
      "                                                        'Gigaword 5 (6B '\n",
      "                                                        'tokens, uncased)',\n",
      "                                        'checksum': '59652db361b7a87ee73834a6c391dfc1',\n",
      "                                        'description': 'Pre-trained vectors '\n",
      "                                                       'based on Wikipedia '\n",
      "                                                       '2014 + Gigaword, 5.6B '\n",
      "                                                       'tokens, 400K vocab, '\n",
      "                                                       'uncased '\n",
      "                                                       '(https://nlp.stanford.edu/projects/glove/).',\n",
      "                                        'file_name': 'glove-wiki-gigaword-200.gz',\n",
      "                                        'file_size': 264336934,\n",
      "                                        'license': 'http://opendatacommons.org/licenses/pddl/',\n",
      "                                        'num_records': 400000,\n",
      "                                        'parameters': {'dimension': 200},\n",
      "                                        'parts': 1,\n",
      "                                        'preprocessing': 'Converted to w2v '\n",
      "                                                         'format with `python '\n",
      "                                                         '-m '\n",
      "                                                         'gensim.scripts.glove2word2vec '\n",
      "                                                         '-i <fname> -o '\n",
      "                                                         'glove-wiki-gigaword-200.txt`.',\n",
      "                                        'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
      "                                                      'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
      "                                        'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-200/__init__.py'},\n",
      "            'glove-wiki-gigaword-300': {'base_dataset': 'Wikipedia 2014 + '\n",
      "                                                        'Gigaword 5 (6B '\n",
      "                                                        'tokens, uncased)',\n",
      "                                        'checksum': '29e9329ac2241937d55b852e8284e89b',\n",
      "                                        'description': 'Pre-trained vectors '\n",
      "                                                       'based on Wikipedia '\n",
      "                                                       '2014 + Gigaword, 5.6B '\n",
      "                                                       'tokens, 400K vocab, '\n",
      "                                                       'uncased '\n",
      "                                                       '(https://nlp.stanford.edu/projects/glove/).',\n",
      "                                        'file_name': 'glove-wiki-gigaword-300.gz',\n",
      "                                        'file_size': 394362229,\n",
      "                                        'license': 'http://opendatacommons.org/licenses/pddl/',\n",
      "                                        'num_records': 400000,\n",
      "                                        'parameters': {'dimension': 300},\n",
      "                                        'parts': 1,\n",
      "                                        'preprocessing': 'Converted to w2v '\n",
      "                                                         'format with `python '\n",
      "                                                         '-m '\n",
      "                                                         'gensim.scripts.glove2word2vec '\n",
      "                                                         '-i <fname> -o '\n",
      "                                                         'glove-wiki-gigaword-300.txt`.',\n",
      "                                        'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
      "                                                      'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
      "                                        'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-300/__init__.py'},\n",
      "            'glove-wiki-gigaword-50': {'base_dataset': 'Wikipedia 2014 + '\n",
      "                                                       'Gigaword 5 (6B tokens, '\n",
      "                                                       'uncased)',\n",
      "                                       'checksum': 'c289bc5d7f2f02c6dc9f2f9b67641813',\n",
      "                                       'description': 'Pre-trained vectors '\n",
      "                                                      'based on Wikipedia 2014 '\n",
      "                                                      '+ Gigaword, 5.6B '\n",
      "                                                      'tokens, 400K vocab, '\n",
      "                                                      'uncased '\n",
      "                                                      '(https://nlp.stanford.edu/projects/glove/).',\n",
      "                                       'file_name': 'glove-wiki-gigaword-50.gz',\n",
      "                                       'file_size': 69182535,\n",
      "                                       'license': 'http://opendatacommons.org/licenses/pddl/',\n",
      "                                       'num_records': 400000,\n",
      "                                       'parameters': {'dimension': 50},\n",
      "                                       'parts': 1,\n",
      "                                       'preprocessing': 'Converted to w2v '\n",
      "                                                        'format with `python '\n",
      "                                                        '-m '\n",
      "                                                        'gensim.scripts.glove2word2vec '\n",
      "                                                        '-i <fname> -o '\n",
      "                                                        'glove-wiki-gigaword-50.txt`.',\n",
      "                                       'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
      "                                                     'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
      "                                       'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-50/__init__.py'},\n",
      "            'word2vec-google-news-300': {'base_dataset': 'Google News (about '\n",
      "                                                         '100 billion words)',\n",
      "                                         'checksum': 'a5e5354d40acb95f9ec66d5977d140ef',\n",
      "                                         'description': 'Pre-trained vectors '\n",
      "                                                        'trained on a part of '\n",
      "                                                        'the Google News '\n",
      "                                                        'dataset (about 100 '\n",
      "                                                        'billion words). The '\n",
      "                                                        'model contains '\n",
      "                                                        '300-dimensional '\n",
      "                                                        'vectors for 3 million '\n",
      "                                                        'words and phrases. '\n",
      "                                                        'The phrases were '\n",
      "                                                        'obtained using a '\n",
      "                                                        'simple data-driven '\n",
      "                                                        'approach described in '\n",
      "                                                        \"'Distributed \"\n",
      "                                                        'Representations of '\n",
      "                                                        'Words and Phrases and '\n",
      "                                                        'their '\n",
      "                                                        \"Compositionality' \"\n",
      "                                                        '(https://code.google.com/archive/p/word2vec/).',\n",
      "                                         'file_name': 'word2vec-google-news-300.gz',\n",
      "                                         'file_size': 1743563840,\n",
      "                                         'license': 'not found',\n",
      "                                         'num_records': 3000000,\n",
      "                                         'parameters': {'dimension': 300},\n",
      "                                         'parts': 1,\n",
      "                                         'read_more': ['https://code.google.com/archive/p/word2vec/',\n",
      "                                                       'https://arxiv.org/abs/1301.3781',\n",
      "                                                       'https://arxiv.org/abs/1310.4546',\n",
      "                                                       'https://www.microsoft.com/en-us/research/publication/linguistic-regularities-in-continuous-space-word-representations/?from=http%3A%2F%2Fresearch.microsoft.com%2Fpubs%2F189726%2Frvecs.pdf'],\n",
      "                                         'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-google-news-300/__init__.py'},\n",
      "            'word2vec-ruscorpora-300': {'base_dataset': 'Russian National '\n",
      "                                                        'Corpus (about 250M '\n",
      "                                                        'words)',\n",
      "                                        'checksum': '9bdebdc8ae6d17d20839dd9b5af10bc4',\n",
      "                                        'description': 'Word2vec Continuous '\n",
      "                                                       'Skipgram vectors '\n",
      "                                                       'trained on full '\n",
      "                                                       'Russian National '\n",
      "                                                       'Corpus (about 250M '\n",
      "                                                       'words). The model '\n",
      "                                                       'contains 185K words.',\n",
      "                                        'file_name': 'word2vec-ruscorpora-300.gz',\n",
      "                                        'file_size': 208427381,\n",
      "                                        'license': 'https://creativecommons.org/licenses/by/4.0/deed.en',\n",
      "                                        'num_records': 184973,\n",
      "                                        'parameters': {'dimension': 300,\n",
      "                                                       'window_size': 10},\n",
      "                                        'parts': 1,\n",
      "                                        'preprocessing': 'The corpus was '\n",
      "                                                         'lemmatized and '\n",
      "                                                         'tagged with '\n",
      "                                                         'Universal PoS',\n",
      "                                        'read_more': ['https://www.academia.edu/24306935/WebVectors_a_Toolkit_for_Building_Web_Interfaces_for_Vector_Semantic_Models',\n",
      "                                                      'http://rusvectores.org/en/',\n",
      "                                                      'https://github.com/RaRe-Technologies/gensim-data/issues/3'],\n",
      "                                        'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-ruscorpora-300/__init__.py'}}}\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "info = api.info() # https://github.com/RaRe-Technologies/gensim-data\n",
    "pprint(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=========-----------------------------------------] 18.2% 302.3/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================--------------------------------] 36.9% 612.8/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===================-------------------------------] 38.7% 643.4/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[====================------------------------------] 40.6% 675.1/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=====================-----------------------------] 42.5% 707.3/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================----------------------------] 44.4% 739.0/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=======================---------------------------] 46.5% 772.5/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[========================--------------------------] 48.4% 805.4/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=========================-------------------------] 50.4% 837.4/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==========================------------------------] 52.3% 869.8/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===========================-----------------------] 54.3% 902.1/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===========================-----------------------] 56.0% 930.5/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==============================--------------------] 61.8% 1028.3/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===============================-------------------] 63.6% 1057.3/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================----------------] 69.9% 1162.9/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=====================================-------------] 74.5% 1238.6/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================================------------] 76.3% 1269.3/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=======================================-----------] 78.3% 1301.6/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==========================================--------] 84.1% 1397.7/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==========================================--------] 85.9% 1427.5/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===========================================-------] 87.8% 1460.3/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[============================================------] 89.8% 1492.7/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=============================================-----] 91.7% 1525.3/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==============================================----] 93.6% 1556.7/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[================================================--] 96.4% 1602.9/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================================================-] 98.2% 1632.5/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-30 14:36:29,413 : INFO : word2vec-google-news-300 downloaded\n",
      "2021-03-30 14:36:31,552 : INFO : loading projection weights from /data/blank54/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n",
      "2021-03-30 14:37:27,918 : INFO : KeyedVectors lifecycle event {'msg': 'loaded (3000000, 300) matrix of type float32 from /data/blank54/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz', 'binary': True, 'encoding': 'utf8', 'datetime': '2021-03-30T14:37:27.918461', 'gensim': '4.0.0', 'python': '3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-107-generic-x86_64-with-debian-stretch-sid', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "\"\"\" It will take a few minutes \"\"\"\n",
    "google_w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.keyedvectors.KeyedVectors'>\n"
     ]
    }
   ],
   "source": [
    "print(type(google_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.keyedvectors.KeyedVectors"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(wb_w2v.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('banks', 0.7440759539604187),\n",
       " ('banking', 0.690161406993866),\n",
       " ('Bank', 0.6698698401451111),\n",
       " ('lender', 0.6342284679412842),\n",
       " ('banker', 0.6092953085899353),\n",
       " ('depositors', 0.6031531691551208),\n",
       " ('mortgage_lender', 0.5797975659370422),\n",
       " ('depositor', 0.5716427564620972),\n",
       " ('BofA', 0.5714625120162964),\n",
       " ('Citibank', 0.5589520335197449)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity Check\n",
    "google_w2v.most_similar('bank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get word vector\n",
    "def word_vector(query, model):\n",
    "    if isinstance(model, gensim.models.word2vec.Word2Vec):\n",
    "        result = model.wv[query]\n",
    "    elif isinstance(model, gensim.models.keyedvectors.KeyedVectors):\n",
    "        result = model[query]\n",
    "    else:\n",
    "        print('No Word2vec model was provided.')\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'google_w2v' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c981a2de8e86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'world'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoogle_w2v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bank'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoogle_w2v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'google_w2v' is not defined"
     ]
    }
   ],
   "source": [
    "pprint(word_vector('world', google_w2v)[:10])\n",
    "pprint(word_vector('bank', google_w2v)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity b/w 2 words\n",
    "def word_similarity(query1, query2, model):\n",
    "    from scipy.spatial.distance import cosine\n",
    "    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html\n",
    "    wv1 = word_vector(query1, model)\n",
    "    wv2 = word_vector(query2, model)\n",
    "    sim = 1 - cosine(wv1, wv2)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = 'world'\n",
    "query2 = 'bank'\n",
    "print(word_similarity(query1, query2, google_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most similar words\n",
    "def similar_words(query, model, k):\n",
    "    if isinstance(model, gensim.models.word2vec.Word2Vec):\n",
    "        return model.wv.most_similar(query, topn=k)\n",
    "    elif isinstance(model, gensim.models.keyedvectors.KeyedVectors):\n",
    "        return model.most_similar(query, topn=k)\n",
    "    else:\n",
    "        print('No Word2vec model was provided.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'world'\n",
    "k = 15\n",
    "print('Most %d similar words with \"%s\"' % (k, query))\n",
    "pprint(similar_words(query, google_w2v, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additive Composition\n",
    "def add_comp(pos1, neg1, pos2, model, k):\n",
    "    \"\"\"\n",
    "    Usage:\n",
    "    Positive word 1 - Negative word 1 + Positive word 2 = Result\n",
    "    Same as Pos1 : Neg1 = Result : Pos2\n",
    "    (e.g. \"Korea\" - \"Seoul\" + \"Tokyo\" = ? ; i.e. Korea:Seoul = ?:Tokyo)\n",
    "    \"\"\"\n",
    "    if isinstance(model, gensim.models.word2vec.Word2Vec):\n",
    "        res = model.wv.most_similar(positive=[pos1, pos2], negative=[neg1], topn=k)\n",
    "    elif isinstance(model, gensim.models.keyedvectors.KeyedVectors):\n",
    "        res = model.most_similar(positive=[pos1, pos2], negative=[neg1], topn=k)\n",
    "    else:\n",
    "        print('No Word2vec model was provided.')\n",
    "        res = None\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos1 = 'Korea'\n",
    "neg1 = 'Seoul'\n",
    "pos2 = 'Tokyo'\n",
    "# pos1 : neg1 = (result) : pos2\n",
    "k = 5\n",
    "print('%d candidate words for the nation whose capital city is %s:' % (k, pos2))\n",
    "pprint(add_comp(pos1, neg1, pos2, google_w2v, k)) # Expecting \"Japan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
