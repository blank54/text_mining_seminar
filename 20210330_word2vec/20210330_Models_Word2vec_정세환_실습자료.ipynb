{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "accomplished-steering",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-unemployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.system('pip install --upgrade gensim') # if Gensim is not installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "constitutional-treasurer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0.0\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "import json\n",
    "from glob import glob\n",
    "import logging\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pprint import pprint # pretty print | https://docs.python.org/ko/3/library/pprint.html\n",
    "\n",
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "lightweight-samuel",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /data/sech/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rising-spanking",
   "metadata": {},
   "source": [
    "# Word2vec from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-framing",
   "metadata": {},
   "source": [
    "## Load Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "combined-italy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_corpus(corpus_dir):\n",
    "    fpaths = glob(corpus_dir + '/*')\n",
    "    corpus = []\n",
    "    for path in fpaths:\n",
    "        with open(path, 'r') as f:\n",
    "            doc = json.load(f)\n",
    "            content = doc['content']\n",
    "            doc_text = word_tokenize(content)\n",
    "            corpus.append(doc_text)\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "exotic-yacht",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" It will take a minute \"\"\"\n",
    "corpus_dir = '/data/sech/workspace/text_mining_seminar/20210330_word2vec/WorldBankNews/'\n",
    "corpus = load_json_corpus(corpus_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "functioning-terry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 9169\n"
     ]
    }
   ],
   "source": [
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-width",
   "metadata": {},
   "source": [
    "## Train Word2vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "turkish-roller",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "younger-current",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-29 14:23:50,656 : INFO : collecting all words and their counts\n",
      "2021-03-29 14:23:50,657 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-03-29 14:23:51,903 : INFO : collected 106683 word types from a corpus of 8194685 raw words and 9169 sentences\n",
      "2021-03-29 14:23:51,904 : INFO : Creating a fresh vocabulary\n",
      "2021-03-29 14:23:52,044 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 29017 unique words (27.19927261138138%% of original 106683, drops 77666)', 'datetime': '2021-03-29T14:23:52.023110', 'gensim': '4.0.0', 'python': '3.8.5 (default, Sep  4 2020, 07:30:14) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-107-generic-x86_64-with-glibc2.10', 'event': 'prepare_vocab'}\n",
      "2021-03-29 14:23:52,045 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 8073911 word corpus (98.52619106164545%% of original 8194685, drops 120774)', 'datetime': '2021-03-29T14:23:52.045543', 'gensim': '4.0.0', 'python': '3.8.5 (default, Sep  4 2020, 07:30:14) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-107-generic-x86_64-with-glibc2.10', 'event': 'prepare_vocab'}\n",
      "2021-03-29 14:23:52,210 : INFO : deleting the raw counts dictionary of 106683 items\n",
      "2021-03-29 14:23:52,215 : INFO : sample=0.001 downsamples 43 most-common words\n",
      "2021-03-29 14:23:52,216 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 5960209.585695938 word corpus (73.8%% of prior 8073911)', 'datetime': '2021-03-29T14:23:52.215971', 'gensim': '4.0.0', 'python': '3.8.5 (default, Sep  4 2020, 07:30:14) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-107-generic-x86_64-with-glibc2.10', 'event': 'prepare_vocab'}\n",
      "2021-03-29 14:23:52,515 : INFO : estimated required memory for 29017 words and 100 dimensions: 37722100 bytes\n",
      "2021-03-29 14:23:52,516 : INFO : resetting layer weights\n",
      "2021-03-29 14:23:52,528 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2021-03-29T14:23:52.528228', 'gensim': '4.0.0', 'python': '3.8.5 (default, Sep  4 2020, 07:30:14) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-107-generic-x86_64-with-glibc2.10', 'event': 'build_vocab'}\n",
      "2021-03-29 14:23:52,528 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 29017 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5', 'datetime': '2021-03-29T14:23:52.528838', 'gensim': '4.0.0', 'python': '3.8.5 (default, Sep  4 2020, 07:30:14) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-107-generic-x86_64-with-glibc2.10', 'event': 'train'}\n",
      "2021-03-29 14:23:53,552 : INFO : EPOCH 1 - PROGRESS: at 9.86% examples, 556816 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-29 14:23:54,562 : INFO : EPOCH 1 - PROGRESS: at 20.12% examples, 578044 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-29 14:23:55,571 : INFO : EPOCH 1 - PROGRESS: at 30.22% examples, 589355 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-29 14:23:56,586 : INFO : EPOCH 1 - PROGRESS: at 40.30% examples, 594549 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-29 14:23:57,586 : INFO : EPOCH 1 - PROGRESS: at 52.85% examples, 625591 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-29 14:23:58,586 : INFO : EPOCH 1 - PROGRESS: at 64.61% examples, 640569 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-29 14:23:59,601 : INFO : EPOCH 1 - PROGRESS: at 75.01% examples, 635451 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-29 14:24:00,608 : INFO : EPOCH 1 - PROGRESS: at 85.20% examples, 630283 words/s, in_qsize 6, out_qsize 1\n",
      "2021-03-29 14:24:01,609 : INFO : EPOCH 1 - PROGRESS: at 95.99% examples, 630547 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-29 14:24:01,992 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-29 14:24:01,996 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-29 14:24:01,999 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-29 14:24:01,999 : INFO : EPOCH - 1 : training on 8194685 raw words (5957844 effective words) took 9.5s, 629770 effective words/s\n",
      "2021-03-29 14:24:03,020 : INFO : EPOCH 2 - PROGRESS: at 10.00% examples, 561527 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-29 14:24:04,021 : INFO : EPOCH 2 - PROGRESS: at 19.99% examples, 575239 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-29 14:24:05,025 : INFO : EPOCH 2 - PROGRESS: at 29.73% examples, 579882 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-29 14:24:06,032 : INFO : EPOCH 2 - PROGRESS: at 39.32% examples, 581181 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-29 14:24:07,050 : INFO : EPOCH 2 - PROGRESS: at 49.42% examples, 581694 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-29 14:24:08,058 : INFO : EPOCH 2 - PROGRESS: at 59.07% examples, 581879 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-29 14:24:09,071 : INFO : EPOCH 2 - PROGRESS: at 68.91% examples, 583753 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-29 14:24:10,082 : INFO : EPOCH 2 - PROGRESS: at 78.79% examples, 583641 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-29 14:24:11,086 : INFO : EPOCH 2 - PROGRESS: at 88.96% examples, 584461 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-29 14:24:12,087 : INFO : EPOCH 2 - PROGRESS: at 99.05% examples, 585007 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-29 14:24:12,174 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-29 14:24:12,182 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-29 14:24:12,187 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-29 14:24:12,187 : INFO : EPOCH - 2 : training on 8194685 raw words (5958685 effective words) took 10.2s, 585039 effective words/s\n",
      "2021-03-29 14:24:13,196 : INFO : EPOCH 3 - PROGRESS: at 10.21% examples, 580148 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-29 14:24:14,200 : INFO : EPOCH 3 - PROGRESS: at 20.34% examples, 588791 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-29 14:24:15,202 : INFO : EPOCH 3 - PROGRESS: at 30.52% examples, 599336 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-29 14:24:16,205 : INFO : EPOCH 3 - PROGRESS: at 43.27% examples, 642493 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-29 14:24:17,205 : INFO : EPOCH 3 - PROGRESS: at 53.43% examples, 636621 words/s, in_qsize 4, out_qsize 1\n",
      "2021-03-29 14:24:18,217 : INFO : EPOCH 3 - PROGRESS: at 63.49% examples, 630732 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-29 14:24:19,227 : INFO : EPOCH 3 - PROGRESS: at 73.81% examples, 628030 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-29 14:24:20,230 : INFO : EPOCH 3 - PROGRESS: at 84.20% examples, 624691 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-29 14:24:21,242 : INFO : EPOCH 3 - PROGRESS: at 94.79% examples, 623765 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-29 14:24:21,729 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-29 14:24:21,730 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-29 14:24:21,737 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-29 14:24:21,738 : INFO : EPOCH - 3 : training on 8194685 raw words (5957732 effective words) took 9.5s, 623978 effective words/s\n",
      "2021-03-29 14:24:22,744 : INFO : EPOCH 4 - PROGRESS: at 10.48% examples, 601942 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-29 14:24:23,752 : INFO : EPOCH 4 - PROGRESS: at 20.99% examples, 611982 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-29 14:24:24,755 : INFO : EPOCH 4 - PROGRESS: at 31.18% examples, 615636 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-29 14:24:25,756 : INFO : EPOCH 4 - PROGRESS: at 41.42% examples, 614991 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-29 14:24:26,771 : INFO : EPOCH 4 - PROGRESS: at 51.90% examples, 616594 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-29 14:24:27,773 : INFO : EPOCH 4 - PROGRESS: at 62.29% examples, 617683 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-29 14:24:28,779 : INFO : EPOCH 4 - PROGRESS: at 72.68% examples, 618478 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-29 14:24:29,780 : INFO : EPOCH 4 - PROGRESS: at 83.29% examples, 618885 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-29 14:24:30,805 : INFO : EPOCH 4 - PROGRESS: at 94.08% examples, 618613 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-29 14:24:31,339 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-29 14:24:31,347 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-29 14:24:31,348 : INFO : worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-29 14:24:31,348 : INFO : EPOCH - 4 : training on 8194685 raw words (5958674 effective words) took 9.6s, 620240 effective words/s\n",
      "2021-03-29 14:24:32,353 : INFO : EPOCH 5 - PROGRESS: at 10.48% examples, 602092 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-29 14:24:33,358 : INFO : EPOCH 5 - PROGRESS: at 20.99% examples, 613069 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-29 14:24:34,358 : INFO : EPOCH 5 - PROGRESS: at 31.18% examples, 616756 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-29 14:24:35,359 : INFO : EPOCH 5 - PROGRESS: at 41.30% examples, 614256 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-29 14:24:36,368 : INFO : EPOCH 5 - PROGRESS: at 51.65% examples, 613584 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-29 14:24:37,391 : INFO : EPOCH 5 - PROGRESS: at 61.88% examples, 613498 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-29 14:24:38,406 : INFO : EPOCH 5 - PROGRESS: at 72.58% examples, 616029 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-29 14:24:39,424 : INFO : EPOCH 5 - PROGRESS: at 83.29% examples, 616314 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-29 14:24:40,440 : INFO : EPOCH 5 - PROGRESS: at 93.91% examples, 616167 words/s, in_qsize 4, out_qsize 1\n",
      "2021-03-29 14:24:40,994 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-29 14:24:40,997 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-29 14:24:40,999 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-29 14:24:41,000 : INFO : EPOCH - 5 : training on 8194685 raw words (5959815 effective words) took 9.6s, 617644 effective words/s\n",
      "2021-03-29 14:24:41,001 : INFO : Word2Vec lifecycle event {'msg': 'training on 40973425 raw words (29792750 effective words) took 48.5s, 614729 effective words/s', 'datetime': '2021-03-29T14:24:41.001684', 'gensim': '4.0.0', 'python': '3.8.5 (default, Sep  4 2020, 07:30:14) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-107-generic-x86_64-with-glibc2.10', 'event': 'train'}\n",
      "2021-03-29 14:24:41,002 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec(vocab=29017, vector_size=100, alpha=0.025)', 'datetime': '2021-03-29T14:24:41.002611', 'gensim': '4.0.0', 'python': '3.8.5 (default, Sep  4 2020, 07:30:14) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-107-generic-x86_64-with-glibc2.10', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "\"\"\" It will take a minute \"\"\"\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "wb_w2v = Word2Vec(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sunrise-supply",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.word2vec.Word2Vec'>\n"
     ]
    }
   ],
   "source": [
    "print(type(w2v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-singapore",
   "metadata": {},
   "source": [
    "## Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "incredible-romance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('banks', 0.6523056030273438),\n",
       " ('transaction', 0.5722414255142212),\n",
       " ('commercial', 0.5686595439910889),\n",
       " ('borrowers', 0.5343312621116638),\n",
       " ('rating', 0.5328773856163025),\n",
       " ('sovereign', 0.520230233669281),\n",
       " ('company', 0.5194247364997864),\n",
       " ('collateral', 0.5083614587783813),\n",
       " ('currency', 0.5027228593826294),\n",
       " ('borrower', 0.4958907961845398)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wb_w2v.wv.most_similar('bank')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-wonder",
   "metadata": {},
   "source": [
    "## Usage Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "round-planet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word vector for \"bank\": (size: 100)\n",
      "array([-1.462332  , -1.8022327 ,  0.53372693,  1.8523256 , -0.84932065,\n",
      "       -1.4591262 ,  1.7800521 ,  0.29778385,  0.61090356,  0.41010308,\n",
      "        0.2648589 , -0.8836652 , -0.23509975, -0.72303736,  0.37825194,\n",
      "        0.9098913 ,  0.15957332, -0.5573116 , -0.16678242, -1.3305517 ,\n",
      "        0.03870054,  1.2751849 , -0.9958456 , -0.34555736, -0.810182  ,\n",
      "       -0.55675745,  1.4750186 ,  0.3988165 , -0.41113296, -0.6253789 ,\n",
      "       -0.73357266, -0.93823725, -0.05289098, -0.2394051 ,  2.6977818 ,\n",
      "        1.0480872 , -1.5274101 , -0.4036461 ,  0.07298851,  2.477908  ,\n",
      "        0.1703974 ,  0.15330207,  0.06536277, -0.8249808 ,  2.2873003 ,\n",
      "       -0.6959409 ,  1.1990219 ,  0.8184906 ,  0.10711139,  1.7417552 ,\n",
      "       -0.34934676,  1.5469879 , -0.91393465,  1.7290417 , -0.9967765 ,\n",
      "        0.48209578, -1.8724929 , -0.02577546, -0.04203504,  0.06223132,\n",
      "        0.92172766,  0.99148655,  0.3224332 , -0.21204974,  2.059879  ,\n",
      "        0.05276413, -0.7487722 , -0.4310792 , -1.7282609 , -0.47790113,\n",
      "       -0.71363   , -0.2515986 , -0.8797547 ,  1.3699933 ,  0.50573117,\n",
      "       -1.356992  ,  0.21838462,  0.89687   ,  0.8192954 ,  1.1828657 ,\n",
      "        1.3544942 , -0.30825138,  0.94023716, -1.40122   , -1.3478398 ,\n",
      "        0.71846133,  1.1567833 ,  0.7418469 , -0.76487356, -0.8317317 ,\n",
      "       -1.3479666 , -2.0462778 , -0.82217366,  0.7675682 ,  1.246774  ,\n",
      "        0.6699448 ,  0.6110745 ,  0.09668009, -0.88612145, -2.900783  ],\n",
      "      dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Get word vector\n",
    "print('word vector for \"bank\": (size: %d)' % len(wb_w2v.wv['bank']))\n",
    "pprint(wb_w2v.wv['bank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "frequent-discharge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine distance b/w \"world\" and \"bank\"\n",
      "0.843868300318718\n",
      "\n",
      "cosine similarity b/w \"world\" and \"bank\"\n",
      "0.15613169968128204\n"
     ]
    }
   ],
   "source": [
    "# Similarity b/w 2 words\n",
    "from scipy.spatial.distance import cosine\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html\n",
    "\n",
    "query1 = 'world'\n",
    "query2 = 'bank'\n",
    "wv1 = wb_w2v.wv[query1]\n",
    "wv2 = wb_w2v.wv[query2]\n",
    "print('cosine distance b/w \"%s\" and \"%s\"' % (query1, query2))\n",
    "print(cosine(wv1, wv2))\n",
    "print()\n",
    "print('cosine similarity b/w \"%s\" and \"%s\"' % (query1, query2))\n",
    "print(1 - cosine(wv1, wv2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fifty-pixel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words with \"world\"\n",
      "[('globe', 0.7241352200508118),\n",
      " ('region', 0.6937957406044006),\n",
      " ('country', 0.6821999549865723),\n",
      " ('continent', 0.6172592043876648),\n",
      " ('nation', 0.5616614818572998),\n",
      " ('global', 0.5160314440727234),\n",
      " ('planet', 0.4878930151462555),\n",
      " ('clock', 0.48342350125312805),\n",
      " ('China', 0.47411882877349854),\n",
      " ('Europe', 0.4573015868663788)]\n",
      "\n",
      "Most 5 similar words with \"bank\"\n",
      "[('banks', 0.6523056030273438),\n",
      " ('transaction', 0.5722414255142212),\n",
      " ('commercial', 0.5686595439910889),\n",
      " ('borrowers', 0.5343312621116638),\n",
      " ('rating', 0.5328773856163025)]\n"
     ]
    }
   ],
   "source": [
    "# Get most similar words\n",
    "query1 = 'world'\n",
    "query2 = 'bank'\n",
    "print('Most similar words with \"%s\"' % query1)\n",
    "pprint(wb_w2v.wv.most_similar('world'))\n",
    "print()\n",
    "print('Most 5 similar words with \"%s\"' % query2)\n",
    "pprint(wb_w2v.wv.most_similar('bank', topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "direct-dispatch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 candidate words for the nation whose capital city is Tokyo:\n",
      "[('Japan', 0.6795076131820679),\n",
      " ('Australia', 0.48621612787246704),\n",
      " ('WTO', 0.4786176383495331),\n",
      " ('IMF', 0.47635480761528015),\n",
      " ('Poland', 0.47211363911628723)]\n"
     ]
    }
   ],
   "source": [
    "# Additive Composition\n",
    "\"\"\"\n",
    "Usage:\n",
    "Positive word 1 - Negative word 1 + Positive word 2 = Result\n",
    "Same as Pos1 : Neg1 = Result : Pos2\n",
    "(e.g. \"Korea\" - \"Seoul\" + \"Tokyo\" = ? ; i.e. Korea:Seoul = ?:Tokyo)\n",
    "\"\"\"\n",
    "\n",
    "pos1 = 'Korea'\n",
    "neg1 = 'Seoul'\n",
    "pos2 = 'Tokyo'\n",
    "# pos1 : neg1 = (result) : pos2\n",
    "k = 5\n",
    "print('%d candidate words for the nation whose capital city is %s:' % (k, pos2))\n",
    "pprint(wb_w2v.wv.most_similar(positive=[pos1, pos2], negative=[neg1], topn=k)) # Expecting \"Japan\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-hudson",
   "metadata": {},
   "source": [
    "# Pretrained Word2vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-romania",
   "metadata": {},
   "source": [
    "## Model Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "automatic-flood",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'corpora': {'20-newsgroups': {'checksum': 'c92fd4f6640a86d5ba89eaad818a9891',\n",
      "                               'description': 'The notorious collection of '\n",
      "                                              'approximately 20,000 newsgroup '\n",
      "                                              'posts, partitioned (nearly) '\n",
      "                                              'evenly across 20 different '\n",
      "                                              'newsgroups.',\n",
      "                               'fields': {'data': '',\n",
      "                                          'id': 'original id inferred from '\n",
      "                                                'folder name',\n",
      "                                          'set': 'marker of original split '\n",
      "                                                 \"(possible values 'train' and \"\n",
      "                                                 \"'test')\",\n",
      "                                          'topic': 'name of topic (20 variant '\n",
      "                                                   'of possible values)'},\n",
      "                               'file_name': '20-newsgroups.gz',\n",
      "                               'file_size': 14483581,\n",
      "                               'license': 'not found',\n",
      "                               'num_records': 18846,\n",
      "                               'parts': 1,\n",
      "                               'read_more': ['http://qwone.com/~jason/20Newsgroups/'],\n",
      "                               'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/20-newsgroups/__init__.py',\n",
      "                               'record_format': 'dict'},\n",
      "             '__testing_matrix-synopsis': {'checksum': '1767ac93a089b43899d54944b07d9dc5',\n",
      "                                           'description': '[THIS IS ONLY FOR '\n",
      "                                                          'TESTING] Synopsis '\n",
      "                                                          'of the movie '\n",
      "                                                          'matrix.',\n",
      "                                           'file_name': '__testing_matrix-synopsis.gz',\n",
      "                                           'parts': 1,\n",
      "                                           'read_more': ['http://www.imdb.com/title/tt0133093/plotsummary?ref_=ttpl_pl_syn#synopsis']},\n",
      "             '__testing_multipart-matrix-synopsis': {'checksum-0': 'c8b0c7d8cf562b1b632c262a173ac338',\n",
      "                                                     'checksum-1': '5ff7fc6818e9a5d9bc1cf12c35ed8b96',\n",
      "                                                     'checksum-2': '966db9d274d125beaac7987202076cba',\n",
      "                                                     'description': '[THIS IS '\n",
      "                                                                    'ONLY FOR '\n",
      "                                                                    'TESTING] '\n",
      "                                                                    'Synopsis '\n",
      "                                                                    'of the '\n",
      "                                                                    'movie '\n",
      "                                                                    'matrix.',\n",
      "                                                     'file_name': '__testing_multipart-matrix-synopsis.gz',\n",
      "                                                     'parts': 3,\n",
      "                                                     'read_more': ['http://www.imdb.com/title/tt0133093/plotsummary?ref_=ttpl_pl_syn#synopsis']},\n",
      "             'fake-news': {'checksum': '5e64e942df13219465927f92dcefd5fe',\n",
      "                           'description': 'News dataset, contains text and '\n",
      "                                          'metadata from 244 websites and '\n",
      "                                          'represents 12,999 posts in total '\n",
      "                                          'from a specific window of 30 days. '\n",
      "                                          'The data was pulled using the '\n",
      "                                          \"webhose.io API, and because it's \"\n",
      "                                          'coming from their crawler, not all '\n",
      "                                          'websites identified by their BS '\n",
      "                                          'Detector are present in this '\n",
      "                                          'dataset. Data sources that were '\n",
      "                                          'missing a label were simply '\n",
      "                                          \"assigned a label of 'bs'. There are \"\n",
      "                                          '(ostensibly) no genuine, reliable, '\n",
      "                                          'or trustworthy news sources '\n",
      "                                          'represented in this dataset (so '\n",
      "                                          \"far), so don't trust anything you \"\n",
      "                                          'read.',\n",
      "                           'fields': {'author': 'author of story',\n",
      "                                      'comments': 'number of Facebook comments',\n",
      "                                      'country': 'data from webhose.io',\n",
      "                                      'crawled': 'date the story was archived',\n",
      "                                      'domain_rank': 'data from webhose.io',\n",
      "                                      'language': 'data from webhose.io',\n",
      "                                      'likes': 'number of Facebook likes',\n",
      "                                      'main_img_url': 'image from story',\n",
      "                                      'ord_in_thread': '',\n",
      "                                      'participants_count': 'number of '\n",
      "                                                            'participants',\n",
      "                                      'published': 'date published',\n",
      "                                      'replies_count': 'number of replies',\n",
      "                                      'shares': 'number of Facebook shares',\n",
      "                                      'site_url': 'site URL from BS detector',\n",
      "                                      'spam_score': 'data from webhose.io',\n",
      "                                      'text': 'text of story',\n",
      "                                      'thread_title': '',\n",
      "                                      'title': 'title of story',\n",
      "                                      'type': 'type of website (label from BS '\n",
      "                                              'detector)',\n",
      "                                      'uuid': 'unique identifier'},\n",
      "                           'file_name': 'fake-news.gz',\n",
      "                           'file_size': 20102776,\n",
      "                           'license': 'https://creativecommons.org/publicdomain/zero/1.0/',\n",
      "                           'num_records': 12999,\n",
      "                           'parts': 1,\n",
      "                           'read_more': ['https://www.kaggle.com/mrisdal/fake-news'],\n",
      "                           'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/fake-news/__init__.py',\n",
      "                           'record_format': 'dict'},\n",
      "             'patent-2017': {'checksum-0': '818501f0b9af62d3b88294d86d509f8f',\n",
      "                             'checksum-1': '66c05635c1d3c7a19b4a335829d09ffa',\n",
      "                             'description': 'Patent Grant Full Text. Contains '\n",
      "                                            'the full text including tables, '\n",
      "                                            \"sequence data and 'in-line' \"\n",
      "                                            'mathematical expressions of each '\n",
      "                                            'patent grant issued in 2017.',\n",
      "                             'file_name': 'patent-2017.gz',\n",
      "                             'file_size': 3087262469,\n",
      "                             'license': 'not found',\n",
      "                             'num_records': 353197,\n",
      "                             'parts': 2,\n",
      "                             'read_more': ['http://patents.reedtech.com/pgrbft.php'],\n",
      "                             'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/patent-2017/__init__.py',\n",
      "                             'record_format': 'dict'},\n",
      "             'quora-duplicate-questions': {'checksum': 'd7cfa7fbc6e2ec71ab74c495586c6365',\n",
      "                                           'description': 'Over 400,000 lines '\n",
      "                                                          'of potential '\n",
      "                                                          'question duplicate '\n",
      "                                                          'pairs. Each line '\n",
      "                                                          'contains IDs for '\n",
      "                                                          'each question in '\n",
      "                                                          'the pair, the full '\n",
      "                                                          'text for each '\n",
      "                                                          'question, and a '\n",
      "                                                          'binary value that '\n",
      "                                                          'indicates whether '\n",
      "                                                          'the line contains a '\n",
      "                                                          'duplicate pair or '\n",
      "                                                          'not.',\n",
      "                                           'fields': {'id': 'the id of a '\n",
      "                                                            'training set '\n",
      "                                                            'question pair',\n",
      "                                                      'is_duplicate': 'the '\n",
      "                                                                      'target '\n",
      "                                                                      'variable, '\n",
      "                                                                      'set to '\n",
      "                                                                      '1 if '\n",
      "                                                                      'question1 '\n",
      "                                                                      'and '\n",
      "                                                                      'question2 '\n",
      "                                                                      'have '\n",
      "                                                                      'essentially '\n",
      "                                                                      'the '\n",
      "                                                                      'same '\n",
      "                                                                      'meaning, '\n",
      "                                                                      'and 0 '\n",
      "                                                                      'otherwise',\n",
      "                                                      'qid1': 'unique ids of '\n",
      "                                                              'each question',\n",
      "                                                      'qid2': 'unique ids of '\n",
      "                                                              'each question',\n",
      "                                                      'question1': 'the full '\n",
      "                                                                   'text of '\n",
      "                                                                   'each '\n",
      "                                                                   'question',\n",
      "                                                      'question2': 'the full '\n",
      "                                                                   'text of '\n",
      "                                                                   'each '\n",
      "                                                                   'question'},\n",
      "                                           'file_name': 'quora-duplicate-questions.gz',\n",
      "                                           'file_size': 21684784,\n",
      "                                           'license': 'probably '\n",
      "                                                      'https://www.quora.com/about/tos',\n",
      "                                           'num_records': 404290,\n",
      "                                           'parts': 1,\n",
      "                                           'read_more': ['https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs'],\n",
      "                                           'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/quora-duplicate-questions/__init__.py',\n",
      "                                           'record_format': 'dict'},\n",
      "             'semeval-2016-2017-task3-subtaskA-unannotated': {'checksum': '2de0e2f2c4f91c66ae4fcf58d50ba816',\n",
      "                                                              'description': 'SemEval '\n",
      "                                                                             '2016 '\n",
      "                                                                             '/ '\n",
      "                                                                             '2017 '\n",
      "                                                                             'Task '\n",
      "                                                                             '3 '\n",
      "                                                                             'Subtask '\n",
      "                                                                             'A '\n",
      "                                                                             'unannotated '\n",
      "                                                                             'dataset '\n",
      "                                                                             'contains '\n",
      "                                                                             '189,941 '\n",
      "                                                                             'questions '\n",
      "                                                                             'and '\n",
      "                                                                             '1,894,456 '\n",
      "                                                                             'comments '\n",
      "                                                                             'in '\n",
      "                                                                             'English '\n",
      "                                                                             'collected '\n",
      "                                                                             'from '\n",
      "                                                                             'the '\n",
      "                                                                             'Community '\n",
      "                                                                             'Question '\n",
      "                                                                             'Answering '\n",
      "                                                                             '(CQA) '\n",
      "                                                                             'web '\n",
      "                                                                             'forum '\n",
      "                                                                             'of '\n",
      "                                                                             'Qatar '\n",
      "                                                                             'Living. '\n",
      "                                                                             'These '\n",
      "                                                                             'can '\n",
      "                                                                             'be '\n",
      "                                                                             'used '\n",
      "                                                                             'as '\n",
      "                                                                             'a '\n",
      "                                                                             'corpus '\n",
      "                                                                             'for '\n",
      "                                                                             'language '\n",
      "                                                                             'modelling.',\n",
      "                                                              'fields': {'RelComments': [{'RELC_DATE': 'date '\n",
      "                                                                                                       'of '\n",
      "                                                                                                       'posting',\n",
      "                                                                                          'RELC_ID': 'comment '\n",
      "                                                                                                     'identifier',\n",
      "                                                                                          'RELC_USERID': 'identifier '\n",
      "                                                                                                         'of '\n",
      "                                                                                                         'the '\n",
      "                                                                                                         'user '\n",
      "                                                                                                         'posting '\n",
      "                                                                                                         'the '\n",
      "                                                                                                         'comment',\n",
      "                                                                                          'RELC_USERNAME': 'name '\n",
      "                                                                                                           'of '\n",
      "                                                                                                           'the '\n",
      "                                                                                                           'user '\n",
      "                                                                                                           'posting '\n",
      "                                                                                                           'the '\n",
      "                                                                                                           'comment',\n",
      "                                                                                          'RelCText': 'text '\n",
      "                                                                                                      'of '\n",
      "                                                                                                      'answer'}],\n",
      "                                                                         'RelQuestion': {'RELQ_CATEGORY': 'question '\n",
      "                                                                                                          'category, '\n",
      "                                                                                                          'according '\n",
      "                                                                                                          'to '\n",
      "                                                                                                          'the '\n",
      "                                                                                                          'Qatar '\n",
      "                                                                                                          'Living '\n",
      "                                                                                                          'taxonomy',\n",
      "                                                                                         'RELQ_DATE': 'date '\n",
      "                                                                                                      'of '\n",
      "                                                                                                      'posting',\n",
      "                                                                                         'RELQ_ID': 'question '\n",
      "                                                                                                    'indentifier',\n",
      "                                                                                         'RELQ_USERID': 'identifier '\n",
      "                                                                                                        'of '\n",
      "                                                                                                        'the '\n",
      "                                                                                                        'user '\n",
      "                                                                                                        'asking '\n",
      "                                                                                                        'the '\n",
      "                                                                                                        'question',\n",
      "                                                                                         'RELQ_USERNAME': 'name '\n",
      "                                                                                                          'of '\n",
      "                                                                                                          'the '\n",
      "                                                                                                          'user '\n",
      "                                                                                                          'asking '\n",
      "                                                                                                          'the '\n",
      "                                                                                                          'question',\n",
      "                                                                                         'RelQBody': 'body '\n",
      "                                                                                                     'of '\n",
      "                                                                                                     'question',\n",
      "                                                                                         'RelQSubject': 'subject '\n",
      "                                                                                                        'of '\n",
      "                                                                                                        'question'},\n",
      "                                                                         'THREAD_SEQUENCE': ''},\n",
      "                                                              'file_name': 'semeval-2016-2017-task3-subtaskA-unannotated.gz',\n",
      "                                                              'file_size': 234373151,\n",
      "                                                              'license': 'These '\n",
      "                                                                         'datasets '\n",
      "                                                                         'are '\n",
      "                                                                         'free '\n",
      "                                                                         'for '\n",
      "                                                                         'general '\n",
      "                                                                         'research '\n",
      "                                                                         'use.',\n",
      "                                                              'num_records': 189941,\n",
      "                                                              'parts': 1,\n",
      "                                                              'read_more': ['http://alt.qcri.org/semeval2016/task3/',\n",
      "                                                                            'http://alt.qcri.org/semeval2016/task3/data/uploads/semeval2016-task3-report.pdf',\n",
      "                                                                            'https://github.com/RaRe-Technologies/gensim-data/issues/18',\n",
      "                                                                            'https://github.com/Witiko/semeval-2016_2017-task3-subtaskA-unannotated-english'],\n",
      "                                                              'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/semeval-2016-2017-task3-subtaskA-unannotated-eng/__init__.py',\n",
      "                                                              'record_format': 'dict'},\n",
      "             'semeval-2016-2017-task3-subtaskBC': {'checksum': '701ea67acd82e75f95e1d8e62fb0ad29',\n",
      "                                                   'description': 'SemEval '\n",
      "                                                                  '2016 / 2017 '\n",
      "                                                                  'Task 3 '\n",
      "                                                                  'Subtask B '\n",
      "                                                                  'and C '\n",
      "                                                                  'datasets '\n",
      "                                                                  'contain '\n",
      "                                                                  'train+development '\n",
      "                                                                  '(317 '\n",
      "                                                                  'original '\n",
      "                                                                  'questions, '\n",
      "                                                                  '3,169 '\n",
      "                                                                  'related '\n",
      "                                                                  'questions, '\n",
      "                                                                  'and 31,690 '\n",
      "                                                                  'comments), '\n",
      "                                                                  'and test '\n",
      "                                                                  'datasets in '\n",
      "                                                                  'English. '\n",
      "                                                                  'The '\n",
      "                                                                  'description '\n",
      "                                                                  'of the '\n",
      "                                                                  'tasks and '\n",
      "                                                                  'the '\n",
      "                                                                  'collected '\n",
      "                                                                  'data is '\n",
      "                                                                  'given in '\n",
      "                                                                  'sections 3 '\n",
      "                                                                  'and 4.1 of '\n",
      "                                                                  'the task '\n",
      "                                                                  'paper '\n",
      "                                                                  'http://alt.qcri.org/semeval2016/task3/data/uploads/semeval2016-task3-report.pdf '\n",
      "                                                                  'linked in '\n",
      "                                                                  'section '\n",
      "                                                                  'Papers of '\n",
      "                                                                  'https://github.com/RaRe-Technologies/gensim-data/issues/18.',\n",
      "                                                   'fields': {'2016-dev': ['...'],\n",
      "                                                              '2016-test': ['...'],\n",
      "                                                              '2016-train': ['...'],\n",
      "                                                              '2017-test': ['...']},\n",
      "                                                   'file_name': 'semeval-2016-2017-task3-subtaskBC.gz',\n",
      "                                                   'file_size': 6344358,\n",
      "                                                   'license': 'All files '\n",
      "                                                              'released for '\n",
      "                                                              'the task are '\n",
      "                                                              'free for '\n",
      "                                                              'general '\n",
      "                                                              'research use',\n",
      "                                                   'num_records': -1,\n",
      "                                                   'parts': 1,\n",
      "                                                   'read_more': ['http://alt.qcri.org/semeval2017/task3/',\n",
      "                                                                 'http://alt.qcri.org/semeval2017/task3/data/uploads/semeval2017-task3.pdf',\n",
      "                                                                 'https://github.com/RaRe-Technologies/gensim-data/issues/18',\n",
      "                                                                 'https://github.com/Witiko/semeval-2016_2017-task3-subtaskB-english'],\n",
      "                                                   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/semeval-2016-2017-task3-subtaskB-eng/__init__.py',\n",
      "                                                   'record_format': 'dict'},\n",
      "             'text8': {'checksum': '68799af40b6bda07dfa47a32612e5364',\n",
      "                       'description': 'First 100,000,000 bytes of plain text '\n",
      "                                      'from Wikipedia. Used for testing '\n",
      "                                      'purposes; see wiki-english-* for proper '\n",
      "                                      'full Wikipedia datasets.',\n",
      "                       'file_name': 'text8.gz',\n",
      "                       'file_size': 33182058,\n",
      "                       'license': 'not found',\n",
      "                       'num_records': 1701,\n",
      "                       'parts': 1,\n",
      "                       'read_more': ['http://mattmahoney.net/dc/textdata.html'],\n",
      "                       'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/text8/__init__.py',\n",
      "                       'record_format': 'list of str (tokens)'},\n",
      "             'wiki-english-20171001': {'checksum-0': 'a7d7d7fd41ea7e2d7fa32ec1bb640d71',\n",
      "                                       'checksum-1': 'b2683e3356ffbca3b6c2dca6e9801f9f',\n",
      "                                       'checksum-2': 'c5cde2a9ae77b3c4ebce804f6df542c2',\n",
      "                                       'checksum-3': '00b71144ed5e3aeeb885de84f7452b81',\n",
      "                                       'description': 'Extracted Wikipedia '\n",
      "                                                      'dump from October 2017. '\n",
      "                                                      'Produced by `python -m '\n",
      "                                                      'gensim.scripts.segment_wiki '\n",
      "                                                      '-f '\n",
      "                                                      'enwiki-20171001-pages-articles.xml.bz2 '\n",
      "                                                      '-o wiki-en.gz`',\n",
      "                                       'fields': {'section_texts': 'list of '\n",
      "                                                                   'body of '\n",
      "                                                                   'sections',\n",
      "                                                  'section_titles': 'list of '\n",
      "                                                                    'titles of '\n",
      "                                                                    'sections',\n",
      "                                                  'title': 'Title of wiki '\n",
      "                                                           'article'},\n",
      "                                       'file_name': 'wiki-english-20171001.gz',\n",
      "                                       'file_size': 6516051717,\n",
      "                                       'license': 'https://dumps.wikimedia.org/legal.html',\n",
      "                                       'num_records': 4924894,\n",
      "                                       'parts': 4,\n",
      "                                       'read_more': ['https://dumps.wikimedia.org/enwiki/20171001/'],\n",
      "                                       'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/wiki-english-20171001/__init__.py',\n",
      "                                       'record_format': 'dict'}},\n",
      " 'models': {'__testing_word2vec-matrix-synopsis': {'checksum': '534dcb8b56a360977a269b7bfc62d124',\n",
      "                                                   'description': '[THIS IS '\n",
      "                                                                  'ONLY FOR '\n",
      "                                                                  'TESTING] '\n",
      "                                                                  'Word '\n",
      "                                                                  'vecrors of '\n",
      "                                                                  'the movie '\n",
      "                                                                  'matrix.',\n",
      "                                                   'file_name': '__testing_word2vec-matrix-synopsis.gz',\n",
      "                                                   'parameters': {'dimensions': 50},\n",
      "                                                   'parts': 1,\n",
      "                                                   'preprocessing': 'Converted '\n",
      "                                                                    'to w2v '\n",
      "                                                                    'using a '\n",
      "                                                                    'preprocessed '\n",
      "                                                                    'corpus. '\n",
      "                                                                    'Converted '\n",
      "                                                                    'to w2v '\n",
      "                                                                    'format '\n",
      "                                                                    'with '\n",
      "                                                                    '`python3.5 '\n",
      "                                                                    '-m '\n",
      "                                                                    'gensim.models.word2vec '\n",
      "                                                                    '-train '\n",
      "                                                                    '<input_filename> '\n",
      "                                                                    '-iter 50 '\n",
      "                                                                    '-output '\n",
      "                                                                    '<output_filename>`.',\n",
      "                                                   'read_more': []},\n",
      "            'conceptnet-numberbatch-17-06-300': {'base_dataset': 'ConceptNet, '\n",
      "                                                                 'word2vec, '\n",
      "                                                                 'GloVe, and '\n",
      "                                                                 'OpenSubtitles '\n",
      "                                                                 '2016',\n",
      "                                                 'checksum': 'fd642d457adcd0ea94da0cd21b150847',\n",
      "                                                 'description': 'ConceptNet '\n",
      "                                                                'Numberbatch '\n",
      "                                                                'consists of '\n",
      "                                                                'state-of-the-art '\n",
      "                                                                'semantic '\n",
      "                                                                'vectors (also '\n",
      "                                                                'known as word '\n",
      "                                                                'embeddings) '\n",
      "                                                                'that can be '\n",
      "                                                                'used directly '\n",
      "                                                                'as a '\n",
      "                                                                'representation '\n",
      "                                                                'of word '\n",
      "                                                                'meanings or '\n",
      "                                                                'as a starting '\n",
      "                                                                'point for '\n",
      "                                                                'further '\n",
      "                                                                'machine '\n",
      "                                                                'learning. '\n",
      "                                                                'ConceptNet '\n",
      "                                                                'Numberbatch '\n",
      "                                                                'is part of '\n",
      "                                                                'the '\n",
      "                                                                'ConceptNet '\n",
      "                                                                'open data '\n",
      "                                                                'project. '\n",
      "                                                                'ConceptNet '\n",
      "                                                                'provides lots '\n",
      "                                                                'of ways to '\n",
      "                                                                'compute with '\n",
      "                                                                'word '\n",
      "                                                                'meanings, one '\n",
      "                                                                'of which is '\n",
      "                                                                'word '\n",
      "                                                                'embeddings. '\n",
      "                                                                'ConceptNet '\n",
      "                                                                'Numberbatch '\n",
      "                                                                'is a snapshot '\n",
      "                                                                'of just the '\n",
      "                                                                'word '\n",
      "                                                                'embeddings. '\n",
      "                                                                'It is built '\n",
      "                                                                'using an '\n",
      "                                                                'ensemble that '\n",
      "                                                                'combines data '\n",
      "                                                                'from '\n",
      "                                                                'ConceptNet, '\n",
      "                                                                'word2vec, '\n",
      "                                                                'GloVe, and '\n",
      "                                                                'OpenSubtitles '\n",
      "                                                                '2016, using a '\n",
      "                                                                'variation on '\n",
      "                                                                'retrofitting.',\n",
      "                                                 'file_name': 'conceptnet-numberbatch-17-06-300.gz',\n",
      "                                                 'file_size': 1225497562,\n",
      "                                                 'license': 'https://github.com/commonsense/conceptnet-numberbatch/blob/master/LICENSE.txt',\n",
      "                                                 'num_records': 1917247,\n",
      "                                                 'parameters': {'dimension': 300},\n",
      "                                                 'parts': 1,\n",
      "                                                 'read_more': ['http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14972',\n",
      "                                                               'https://github.com/commonsense/conceptnet-numberbatch',\n",
      "                                                               'http://conceptnet.io/'],\n",
      "                                                 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/conceptnet-numberbatch-17-06-300/__init__.py'},\n",
      "            'fasttext-wiki-news-subwords-300': {'base_dataset': 'Wikipedia '\n",
      "                                                                '2017, UMBC '\n",
      "                                                                'webbase '\n",
      "                                                                'corpus and '\n",
      "                                                                'statmt.org '\n",
      "                                                                'news dataset '\n",
      "                                                                '(16B tokens)',\n",
      "                                                'checksum': 'de2bb3a20c46ce65c9c131e1ad9a77af',\n",
      "                                                'description': '1 million word '\n",
      "                                                               'vectors '\n",
      "                                                               'trained on '\n",
      "                                                               'Wikipedia '\n",
      "                                                               '2017, UMBC '\n",
      "                                                               'webbase corpus '\n",
      "                                                               'and statmt.org '\n",
      "                                                               'news dataset '\n",
      "                                                               '(16B tokens).',\n",
      "                                                'file_name': 'fasttext-wiki-news-subwords-300.gz',\n",
      "                                                'file_size': 1005007116,\n",
      "                                                'license': 'https://creativecommons.org/licenses/by-sa/3.0/',\n",
      "                                                'num_records': 999999,\n",
      "                                                'parameters': {'dimension': 300},\n",
      "                                                'parts': 1,\n",
      "                                                'read_more': ['https://fasttext.cc/docs/en/english-vectors.html',\n",
      "                                                              'https://arxiv.org/abs/1712.09405',\n",
      "                                                              'https://arxiv.org/abs/1607.01759'],\n",
      "                                                'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/fasttext-wiki-news-subwords-300/__init__.py'},\n",
      "            'glove-twitter-100': {'base_dataset': 'Twitter (2B tweets, 27B '\n",
      "                                                  'tokens, 1.2M vocab, '\n",
      "                                                  'uncased)',\n",
      "                                  'checksum': 'b04f7bed38756d64cf55b58ce7e97b15',\n",
      "                                  'description': 'Pre-trained vectors based '\n",
      "                                                 'on  2B tweets, 27B tokens, '\n",
      "                                                 '1.2M vocab, uncased '\n",
      "                                                 '(https://nlp.stanford.edu/projects/glove/)',\n",
      "                                  'file_name': 'glove-twitter-100.gz',\n",
      "                                  'file_size': 405932991,\n",
      "                                  'license': 'http://opendatacommons.org/licenses/pddl/',\n",
      "                                  'num_records': 1193514,\n",
      "                                  'parameters': {'dimension': 100},\n",
      "                                  'parts': 1,\n",
      "                                  'preprocessing': 'Converted to w2v format '\n",
      "                                                   'with `python -m '\n",
      "                                                   'gensim.scripts.glove2word2vec '\n",
      "                                                   '-i <fname> -o '\n",
      "                                                   'glove-twitter-100.txt`.',\n",
      "                                  'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
      "                                                'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
      "                                  'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-100/__init__.py'},\n",
      "            'glove-twitter-200': {'base_dataset': 'Twitter (2B tweets, 27B '\n",
      "                                                  'tokens, 1.2M vocab, '\n",
      "                                                  'uncased)',\n",
      "                                  'checksum': 'e52e8392d1860b95d5308a525817d8f9',\n",
      "                                  'description': 'Pre-trained vectors based on '\n",
      "                                                 '2B tweets, 27B tokens, 1.2M '\n",
      "                                                 'vocab, uncased '\n",
      "                                                 '(https://nlp.stanford.edu/projects/glove/).',\n",
      "                                  'file_name': 'glove-twitter-200.gz',\n",
      "                                  'file_size': 795373100,\n",
      "                                  'license': 'http://opendatacommons.org/licenses/pddl/',\n",
      "                                  'num_records': 1193514,\n",
      "                                  'parameters': {'dimension': 200},\n",
      "                                  'parts': 1,\n",
      "                                  'preprocessing': 'Converted to w2v format '\n",
      "                                                   'with `python -m '\n",
      "                                                   'gensim.scripts.glove2word2vec '\n",
      "                                                   '-i <fname> -o '\n",
      "                                                   'glove-twitter-200.txt`.',\n",
      "                                  'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
      "                                                'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
      "                                  'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-200/__init__.py'},\n",
      "            'glove-twitter-25': {'base_dataset': 'Twitter (2B tweets, 27B '\n",
      "                                                 'tokens, 1.2M vocab, uncased)',\n",
      "                                 'checksum': '50db0211d7e7a2dcd362c6b774762793',\n",
      "                                 'description': 'Pre-trained vectors based on '\n",
      "                                                '2B tweets, 27B tokens, 1.2M '\n",
      "                                                'vocab, uncased '\n",
      "                                                '(https://nlp.stanford.edu/projects/glove/).',\n",
      "                                 'file_name': 'glove-twitter-25.gz',\n",
      "                                 'file_size': 109885004,\n",
      "                                 'license': 'http://opendatacommons.org/licenses/pddl/',\n",
      "                                 'num_records': 1193514,\n",
      "                                 'parameters': {'dimension': 25},\n",
      "                                 'parts': 1,\n",
      "                                 'preprocessing': 'Converted to w2v format '\n",
      "                                                  'with `python -m '\n",
      "                                                  'gensim.scripts.glove2word2vec '\n",
      "                                                  '-i <fname> -o '\n",
      "                                                  'glove-twitter-25.txt`.',\n",
      "                                 'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
      "                                               'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
      "                                 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-25/__init__.py'},\n",
      "            'glove-twitter-50': {'base_dataset': 'Twitter (2B tweets, 27B '\n",
      "                                                 'tokens, 1.2M vocab, uncased)',\n",
      "                                 'checksum': 'c168f18641f8c8a00fe30984c4799b2b',\n",
      "                                 'description': 'Pre-trained vectors based on '\n",
      "                                                '2B tweets, 27B tokens, 1.2M '\n",
      "                                                'vocab, uncased '\n",
      "                                                '(https://nlp.stanford.edu/projects/glove/)',\n",
      "                                 'file_name': 'glove-twitter-50.gz',\n",
      "                                 'file_size': 209216938,\n",
      "                                 'license': 'http://opendatacommons.org/licenses/pddl/',\n",
      "                                 'num_records': 1193514,\n",
      "                                 'parameters': {'dimension': 50},\n",
      "                                 'parts': 1,\n",
      "                                 'preprocessing': 'Converted to w2v format '\n",
      "                                                  'with `python -m '\n",
      "                                                  'gensim.scripts.glove2word2vec '\n",
      "                                                  '-i <fname> -o '\n",
      "                                                  'glove-twitter-50.txt`.',\n",
      "                                 'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
      "                                               'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
      "                                 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-50/__init__.py'},\n",
      "            'glove-wiki-gigaword-100': {'base_dataset': 'Wikipedia 2014 + '\n",
      "                                                        'Gigaword 5 (6B '\n",
      "                                                        'tokens, uncased)',\n",
      "                                        'checksum': '40ec481866001177b8cd4cb0df92924f',\n",
      "                                        'description': 'Pre-trained vectors '\n",
      "                                                       'based on Wikipedia '\n",
      "                                                       '2014 + Gigaword 5.6B '\n",
      "                                                       'tokens, 400K vocab, '\n",
      "                                                       'uncased '\n",
      "                                                       '(https://nlp.stanford.edu/projects/glove/).',\n",
      "                                        'file_name': 'glove-wiki-gigaword-100.gz',\n",
      "                                        'file_size': 134300434,\n",
      "                                        'license': 'http://opendatacommons.org/licenses/pddl/',\n",
      "                                        'num_records': 400000,\n",
      "                                        'parameters': {'dimension': 100},\n",
      "                                        'parts': 1,\n",
      "                                        'preprocessing': 'Converted to w2v '\n",
      "                                                         'format with `python '\n",
      "                                                         '-m '\n",
      "                                                         'gensim.scripts.glove2word2vec '\n",
      "                                                         '-i <fname> -o '\n",
      "                                                         'glove-wiki-gigaword-100.txt`.',\n",
      "                                        'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
      "                                                      'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
      "                                        'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-100/__init__.py'},\n",
      "            'glove-wiki-gigaword-200': {'base_dataset': 'Wikipedia 2014 + '\n",
      "                                                        'Gigaword 5 (6B '\n",
      "                                                        'tokens, uncased)',\n",
      "                                        'checksum': '59652db361b7a87ee73834a6c391dfc1',\n",
      "                                        'description': 'Pre-trained vectors '\n",
      "                                                       'based on Wikipedia '\n",
      "                                                       '2014 + Gigaword, 5.6B '\n",
      "                                                       'tokens, 400K vocab, '\n",
      "                                                       'uncased '\n",
      "                                                       '(https://nlp.stanford.edu/projects/glove/).',\n",
      "                                        'file_name': 'glove-wiki-gigaword-200.gz',\n",
      "                                        'file_size': 264336934,\n",
      "                                        'license': 'http://opendatacommons.org/licenses/pddl/',\n",
      "                                        'num_records': 400000,\n",
      "                                        'parameters': {'dimension': 200},\n",
      "                                        'parts': 1,\n",
      "                                        'preprocessing': 'Converted to w2v '\n",
      "                                                         'format with `python '\n",
      "                                                         '-m '\n",
      "                                                         'gensim.scripts.glove2word2vec '\n",
      "                                                         '-i <fname> -o '\n",
      "                                                         'glove-wiki-gigaword-200.txt`.',\n",
      "                                        'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
      "                                                      'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
      "                                        'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-200/__init__.py'},\n",
      "            'glove-wiki-gigaword-300': {'base_dataset': 'Wikipedia 2014 + '\n",
      "                                                        'Gigaword 5 (6B '\n",
      "                                                        'tokens, uncased)',\n",
      "                                        'checksum': '29e9329ac2241937d55b852e8284e89b',\n",
      "                                        'description': 'Pre-trained vectors '\n",
      "                                                       'based on Wikipedia '\n",
      "                                                       '2014 + Gigaword, 5.6B '\n",
      "                                                       'tokens, 400K vocab, '\n",
      "                                                       'uncased '\n",
      "                                                       '(https://nlp.stanford.edu/projects/glove/).',\n",
      "                                        'file_name': 'glove-wiki-gigaword-300.gz',\n",
      "                                        'file_size': 394362229,\n",
      "                                        'license': 'http://opendatacommons.org/licenses/pddl/',\n",
      "                                        'num_records': 400000,\n",
      "                                        'parameters': {'dimension': 300},\n",
      "                                        'parts': 1,\n",
      "                                        'preprocessing': 'Converted to w2v '\n",
      "                                                         'format with `python '\n",
      "                                                         '-m '\n",
      "                                                         'gensim.scripts.glove2word2vec '\n",
      "                                                         '-i <fname> -o '\n",
      "                                                         'glove-wiki-gigaword-300.txt`.',\n",
      "                                        'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
      "                                                      'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
      "                                        'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-300/__init__.py'},\n",
      "            'glove-wiki-gigaword-50': {'base_dataset': 'Wikipedia 2014 + '\n",
      "                                                       'Gigaword 5 (6B tokens, '\n",
      "                                                       'uncased)',\n",
      "                                       'checksum': 'c289bc5d7f2f02c6dc9f2f9b67641813',\n",
      "                                       'description': 'Pre-trained vectors '\n",
      "                                                      'based on Wikipedia 2014 '\n",
      "                                                      '+ Gigaword, 5.6B '\n",
      "                                                      'tokens, 400K vocab, '\n",
      "                                                      'uncased '\n",
      "                                                      '(https://nlp.stanford.edu/projects/glove/).',\n",
      "                                       'file_name': 'glove-wiki-gigaword-50.gz',\n",
      "                                       'file_size': 69182535,\n",
      "                                       'license': 'http://opendatacommons.org/licenses/pddl/',\n",
      "                                       'num_records': 400000,\n",
      "                                       'parameters': {'dimension': 50},\n",
      "                                       'parts': 1,\n",
      "                                       'preprocessing': 'Converted to w2v '\n",
      "                                                        'format with `python '\n",
      "                                                        '-m '\n",
      "                                                        'gensim.scripts.glove2word2vec '\n",
      "                                                        '-i <fname> -o '\n",
      "                                                        'glove-wiki-gigaword-50.txt`.',\n",
      "                                       'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
      "                                                     'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
      "                                       'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-50/__init__.py'},\n",
      "            'word2vec-google-news-300': {'base_dataset': 'Google News (about '\n",
      "                                                         '100 billion words)',\n",
      "                                         'checksum': 'a5e5354d40acb95f9ec66d5977d140ef',\n",
      "                                         'description': 'Pre-trained vectors '\n",
      "                                                        'trained on a part of '\n",
      "                                                        'the Google News '\n",
      "                                                        'dataset (about 100 '\n",
      "                                                        'billion words). The '\n",
      "                                                        'model contains '\n",
      "                                                        '300-dimensional '\n",
      "                                                        'vectors for 3 million '\n",
      "                                                        'words and phrases. '\n",
      "                                                        'The phrases were '\n",
      "                                                        'obtained using a '\n",
      "                                                        'simple data-driven '\n",
      "                                                        'approach described in '\n",
      "                                                        \"'Distributed \"\n",
      "                                                        'Representations of '\n",
      "                                                        'Words and Phrases and '\n",
      "                                                        'their '\n",
      "                                                        \"Compositionality' \"\n",
      "                                                        '(https://code.google.com/archive/p/word2vec/).',\n",
      "                                         'file_name': 'word2vec-google-news-300.gz',\n",
      "                                         'file_size': 1743563840,\n",
      "                                         'license': 'not found',\n",
      "                                         'num_records': 3000000,\n",
      "                                         'parameters': {'dimension': 300},\n",
      "                                         'parts': 1,\n",
      "                                         'read_more': ['https://code.google.com/archive/p/word2vec/',\n",
      "                                                       'https://arxiv.org/abs/1301.3781',\n",
      "                                                       'https://arxiv.org/abs/1310.4546',\n",
      "                                                       'https://www.microsoft.com/en-us/research/publication/linguistic-regularities-in-continuous-space-word-representations/?from=http%3A%2F%2Fresearch.microsoft.com%2Fpubs%2F189726%2Frvecs.pdf'],\n",
      "                                         'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-google-news-300/__init__.py'},\n",
      "            'word2vec-ruscorpora-300': {'base_dataset': 'Russian National '\n",
      "                                                        'Corpus (about 250M '\n",
      "                                                        'words)',\n",
      "                                        'checksum': '9bdebdc8ae6d17d20839dd9b5af10bc4',\n",
      "                                        'description': 'Word2vec Continuous '\n",
      "                                                       'Skipgram vectors '\n",
      "                                                       'trained on full '\n",
      "                                                       'Russian National '\n",
      "                                                       'Corpus (about 250M '\n",
      "                                                       'words). The model '\n",
      "                                                       'contains 185K words.',\n",
      "                                        'file_name': 'word2vec-ruscorpora-300.gz',\n",
      "                                        'file_size': 208427381,\n",
      "                                        'license': 'https://creativecommons.org/licenses/by/4.0/deed.en',\n",
      "                                        'num_records': 184973,\n",
      "                                        'parameters': {'dimension': 300,\n",
      "                                                       'window_size': 10},\n",
      "                                        'parts': 1,\n",
      "                                        'preprocessing': 'The corpus was '\n",
      "                                                         'lemmatized and '\n",
      "                                                         'tagged with '\n",
      "                                                         'Universal PoS',\n",
      "                                        'read_more': ['https://www.academia.edu/24306935/WebVectors_a_Toolkit_for_Building_Web_Interfaces_for_Vector_Semantic_Models',\n",
      "                                                      'http://rusvectores.org/en/',\n",
      "                                                      'https://github.com/RaRe-Technologies/gensim-data/issues/3'],\n",
      "                                        'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-ruscorpora-300/__init__.py'}}}\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "info = api.info() # https://github.com/RaRe-Technologies/gensim-data\n",
    "pprint(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "hidden-breakdown",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-29 14:33:19,385 : INFO : loading projection weights from /data/sech/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n",
      "2021-03-29 14:34:00,090 : INFO : KeyedVectors lifecycle event {'msg': 'loaded (3000000, 300) matrix of type float32 from /data/sech/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz', 'binary': True, 'encoding': 'utf8', 'datetime': '2021-03-29T14:34:00.090172', 'gensim': '4.0.0', 'python': '3.8.5 (default, Sep  4 2020, 07:30:14) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-107-generic-x86_64-with-glibc2.10', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "\"\"\" It will take a few minutes \"\"\"\n",
    "google_w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "israeli-latex",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.keyedvectors.KeyedVectors'>\n"
     ]
    }
   ],
   "source": [
    "print(type(google_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "english-newfoundland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('banks', 0.7440759539604187),\n",
       " ('banking', 0.690161406993866),\n",
       " ('Bank', 0.6698699593544006),\n",
       " ('lender', 0.6342284083366394),\n",
       " ('banker', 0.6092954277992249),\n",
       " ('depositors', 0.6031532287597656),\n",
       " ('mortgage_lender', 0.5797975659370422),\n",
       " ('depositor', 0.5716428756713867),\n",
       " ('BofA', 0.5714625120162964),\n",
       " ('Citibank', 0.5589520931243896)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity Check\n",
    "google_w2v.most_similar('bank')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-america",
   "metadata": {},
   "source": [
    "## Usage Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "opened-richards",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get word vector\n",
    "def word_vector(query, model):\n",
    "    if isinstance(model, gensim.models.word2vec.Word2Vec):\n",
    "        result = model.wv[query]\n",
    "    elif isinstance(model, gensim.models.keyedvectors.KeyedVectors):\n",
    "        result = model[query]\n",
    "    else:\n",
    "        print('No Word2vec model was provided.')\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "vanilla-render",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([-0.06396484,  0.06835938,  0.22460938,  0.13183594, -0.05957031,\n",
      "        0.03881836,  0.07568359, -0.14160156,  0.07080078,  0.15136719],\n",
      "      dtype=float32)\n",
      "array([ 0.02197266,  0.13476562, -0.05786133,  0.05566406,  0.09912109,\n",
      "       -0.140625  , -0.0030365 ,  0.01879883,  0.25390625, -0.04882812],\n",
      "      dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "pprint(word_vector('world', google_w2v)[:10])\n",
    "pprint(word_vector('bank', google_w2v)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "damaged-protocol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity b/w 2 words\n",
    "def word_similarity(query1, query2, model):\n",
    "    from scipy.spatial.distance import cosine\n",
    "    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html\n",
    "    wv1 = word_vector(query1, model)\n",
    "    wv2 = word_vector(query2, model)\n",
    "    sim = 1 - cosine(wv1, wv2)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "advanced-claim",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.01078906748443842\n"
     ]
    }
   ],
   "source": [
    "query1 = 'world'\n",
    "query2 = 'bank'\n",
    "print(word_similarity(query1, query2, google_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "involved-eleven",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most similar words\n",
    "def similar_words(query, model, k):\n",
    "    if isinstance(model, gensim.models.word2vec.Word2Vec):\n",
    "        return model.wv.most_similar(query, topn=k)\n",
    "    elif isinstance(model, gensim.models.keyedvectors.KeyedVectors):\n",
    "        return model.most_similar(query, topn=k)\n",
    "    else:\n",
    "        print('No Word2vec model was provided.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "white-villa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most 15 similar words with \"world\"\n",
      "[('globe', 0.6945997476577759),\n",
      " ('theworld', 0.6902236342430115),\n",
      " ('country', 0.5980385541915894),\n",
      " ('continent', 0.5966995358467102),\n",
      " ('world__', 0.5897718071937561),\n",
      " ('nation', 0.5760580897331238),\n",
      " ('global', 0.5744006037712097),\n",
      " ('worldwide', 0.5641196966171265),\n",
      " ('United_States', 0.544048011302948),\n",
      " ('globally', 0.5411289930343628),\n",
      " ('worlds', 0.5359798669815063),\n",
      " ('worlds', 0.5294704437255859),\n",
      " ('America', 0.5279377102851868),\n",
      " ('Europe', 0.5250756740570068),\n",
      " ('planet', 0.5170571804046631)]\n"
     ]
    }
   ],
   "source": [
    "query = 'world'\n",
    "k = 15\n",
    "print('Most %d similar words with \"%s\"' % (k, query))\n",
    "pprint(similar_words(query, google_w2v, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "processed-screen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additive Composition\n",
    "def add_comp(pos1, neg1, pos2, model, k):\n",
    "    \"\"\"\n",
    "    Usage:\n",
    "    Positive word 1 - Negative word 1 + Positive word 2 = Result\n",
    "    Same as Pos1 : Neg1 = Result : Pos2\n",
    "    (e.g. \"Korea\" - \"Seoul\" + \"Tokyo\" = ? ; i.e. Korea:Seoul = ?:Tokyo)\n",
    "    \"\"\"\n",
    "    if isinstance(model, gensim.models.word2vec.Word2Vec):\n",
    "        res = model.wv.most_similar(positive=[pos1, pos2], negative=[neg1], topn=k)\n",
    "    elif isinstance(model, gensim.models.keyedvectors.KeyedVectors):\n",
    "        res = model.most_similar(positive=[pos1, pos2], negative=[neg1], topn=k)\n",
    "    else:\n",
    "        print('No Word2vec model was provided.')\n",
    "        res = None\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dimensional-smell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 candidate words for the nation whose capital city is Tokyo:\n",
      "[('Japan', 0.8032678365707397),\n",
      " ('Japanese', 0.6562737822532654),\n",
      " ('Japans', 0.6080487370491028),\n",
      " ('Nippon', 0.5482696890830994),\n",
      " ('Toshiya', 0.5421562790870667)]\n"
     ]
    }
   ],
   "source": [
    "pos1 = 'Korea'\n",
    "neg1 = 'Seoul'\n",
    "pos2 = 'Tokyo'\n",
    "# pos1 : neg1 = (result) : pos2\n",
    "k = 5\n",
    "print('%d candidate words for the nation whose capital city is %s:' % (k, pos2))\n",
    "pprint(add_comp(pos1, neg1, pos2, google_w2v, k)) # Expecting \"Japan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excellent-grant",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
